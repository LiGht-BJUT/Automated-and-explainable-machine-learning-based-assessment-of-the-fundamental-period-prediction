{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "357b095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4040213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Storeys(NS)</th>\n",
       "      <th>Height of Structure(HS)</th>\n",
       "      <th>Number of Spans(HSP)</th>\n",
       "      <th>Length of Spans (LS)</th>\n",
       "      <th>Opening percentage (OP)</th>\n",
       "      <th>Masonry wall Stiffeness Et (MS)</th>\n",
       "      <th>Period (P)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.092970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.159750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.094770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>100</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.156589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.106350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>22</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>50</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>22</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>75</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>22</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>22</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.602680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>22</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>100</td>\n",
       "      <td>25.00</td>\n",
       "      <td>3.474000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4026 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Number of Storeys(NS)  Height of Structure(HS)  Number of Spans(HSP)  \\\n",
       "0                         1                        3                     2   \n",
       "1                         1                        3                     2   \n",
       "2                         1                        3                     2   \n",
       "3                         1                        3                     2   \n",
       "4                         1                        3                     2   \n",
       "...                     ...                      ...                   ...   \n",
       "4021                     22                       66                     6   \n",
       "4022                     22                       66                     6   \n",
       "4023                     22                       66                     6   \n",
       "4024                     22                       66                     6   \n",
       "4025                     22                       66                     6   \n",
       "\n",
       "      Length of Spans (LS)  Opening percentage (OP)  \\\n",
       "0                      3.0                        0   \n",
       "1                      3.0                      100   \n",
       "2                      4.5                        0   \n",
       "3                      4.5                      100   \n",
       "4                      6.0                        0   \n",
       "...                    ...                      ...   \n",
       "4021                   6.0                       50   \n",
       "4022                   6.0                       75   \n",
       "4023                   6.0                      100   \n",
       "4024                   7.5                        0   \n",
       "4025                   7.5                      100   \n",
       "\n",
       "      Masonry wall Stiffeness Et (MS)  Period (P)  \n",
       "0                                2.25    0.092970  \n",
       "1                                2.25    0.159750  \n",
       "2                                2.25    0.094770  \n",
       "3                                2.25    0.156589  \n",
       "4                                2.25    0.106350  \n",
       "...                               ...         ...  \n",
       "4021                            25.00    1.628000  \n",
       "4022                            25.00    2.714000  \n",
       "4023                            25.00    2.879000  \n",
       "4024                            25.00    0.602680  \n",
       "4025                            25.00    3.474000  \n",
       "\n",
       "[4026 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel(r\"../dataset/data.xlsx\",sheet_name=\"Sheet1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e275a747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4026, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe2e8e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Storeys(NS)</th>\n",
       "      <th>Height of Structure(HS)</th>\n",
       "      <th>Number of Spans(HSP)</th>\n",
       "      <th>Length of Spans (LS)</th>\n",
       "      <th>Opening percentage (OP)</th>\n",
       "      <th>Masonry wall Stiffeness Et (MS)</th>\n",
       "      <th>Period (P)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.092970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.159750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.094770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>100</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.156589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.106350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Storeys(NS)  Height of Structure(HS)  Number of Spans(HSP)  \\\n",
       "0                      1                        3                     2   \n",
       "1                      1                        3                     2   \n",
       "2                      1                        3                     2   \n",
       "3                      1                        3                     2   \n",
       "4                      1                        3                     2   \n",
       "\n",
       "   Length of Spans (LS)  Opening percentage (OP)  \\\n",
       "0                   3.0                        0   \n",
       "1                   3.0                      100   \n",
       "2                   4.5                        0   \n",
       "3                   4.5                      100   \n",
       "4                   6.0                        0   \n",
       "\n",
       "   Masonry wall Stiffeness Et (MS)  Period (P)  \n",
       "0                             2.25    0.092970  \n",
       "1                             2.25    0.159750  \n",
       "2                             2.25    0.094770  \n",
       "3                             2.25    0.156589  \n",
       "4                             2.25    0.106350  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e9bfd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Number of Storeys(NS)  Height of Structure(HS)  Number of Spans(HSP)  \\\n",
      "0                       0.0                      0.0                   0.0   \n",
      "1                       0.0                      0.0                   0.0   \n",
      "2                       0.0                      0.0                   0.0   \n",
      "3                       0.0                      0.0                   0.0   \n",
      "4                       0.0                      0.0                   0.0   \n",
      "...                     ...                      ...                   ...   \n",
      "4021                    1.0                      1.0                   1.0   \n",
      "4022                    1.0                      1.0                   1.0   \n",
      "4023                    1.0                      1.0                   1.0   \n",
      "4024                    1.0                      1.0                   1.0   \n",
      "4025                    1.0                      1.0                   1.0   \n",
      "\n",
      "      Length of Spans (LS)  Opening percentage (OP)  \\\n",
      "0                 0.000000                     0.00   \n",
      "1                 0.000000                     1.00   \n",
      "2                 0.333333                     0.00   \n",
      "3                 0.333333                     1.00   \n",
      "4                 0.666667                     0.00   \n",
      "...                    ...                      ...   \n",
      "4021              0.666667                     0.50   \n",
      "4022              0.666667                     0.75   \n",
      "4023              0.666667                     1.00   \n",
      "4024              1.000000                     0.00   \n",
      "4025              1.000000                     1.00   \n",
      "\n",
      "      Masonry wall Stiffeness Et (MS)  Period (P)  \n",
      "0                                 0.0    0.015051  \n",
      "1                                 0.0    0.033991  \n",
      "2                                 0.0    0.015562  \n",
      "3                                 0.0    0.033095  \n",
      "4                                 0.0    0.018846  \n",
      "...                               ...         ...  \n",
      "4021                              1.0    0.450410  \n",
      "4022                              1.0    0.758416  \n",
      "4023                              1.0    0.805213  \n",
      "4024                              1.0    0.159613  \n",
      "4025                              1.0    0.973964  \n",
      "\n",
      "[4026 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "print(normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f75398b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Number of Storeys(NS)', 'Height of Structure(HS)',\n",
       "       'Number of Spans(HSP)', 'Length of Spans (LS)',\n",
       "       'Opening percentage (OP)', 'Masonry wall Stiffeness Et (MS)',\n",
       "       'Period (P)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f15b32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=normalized_df.drop(\"Period (P)\" ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd30344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=normalized_df[\"Period (P)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1e09009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bddc88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8042ce6",
   "metadata": {},
   "source": [
    "# NGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9229f554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:14:02,214] A new study created in memory with name: no-name-5e611008-90a0-4af9-9c91-a570cf6fa053\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1707 val_loss=0.0000 scale=1.0000 norm=0.4092\n",
      "[iter 200] loss=-3.3411 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.5034 val_loss=0.0000 scale=1.0000 norm=0.4158\n",
      "[iter 400] loss=-3.5880 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.6211 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6224 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 700] loss=-3.6588 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.6819 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.6870 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7150 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7174 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7182 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7164 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7286 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7470 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-3.7513 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-3.7732 val_loss=0.0000 scale=0.2500 norm=0.1023\n",
      "[iter 1800] loss=-3.8018 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.8012 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8005 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.7996 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.8202 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.8199 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.8272 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-3.8465 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-3.8657 val_loss=0.0000 scale=1.0000 norm=0.4069\n",
      "[iter 2700] loss=-3.8864 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-3.8867 val_loss=0.0000 scale=1.0000 norm=0.4070\n",
      "[iter 2900] loss=-3.8965 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3000] loss=-3.9160 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:15:00,320] Trial 0 finished with value: 7.395342471927014e-05 and parameters: {'n_estimators': 3075, 'learning_rate': 0.49942193497360154}. Best is trial 0 with value: 7.395342471927014e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1048 val_loss=0.0000 scale=0.5000 norm=0.2000\n",
      "[iter 200] loss=-3.4033 val_loss=0.0000 scale=0.2500 norm=0.1020\n",
      "[iter 300] loss=-3.5242 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.5238 val_loss=0.0000 scale=0.1250 norm=0.0514\n",
      "[iter 500] loss=-3.6064 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6061 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.6408 val_loss=0.0000 scale=1.0000 norm=0.4095\n",
      "[iter 800] loss=-3.6822 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.6936 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6934 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7021 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7036 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7051 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7049 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7047 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.7065 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-3.7087 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.7084 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.7333 val_loss=0.0000 scale=0.0312 norm=0.0127\n",
      "[iter 2000] loss=-3.7753 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2100] loss=-3.7897 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2200] loss=-3.8002 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2300] loss=-3.8020 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.8376 val_loss=0.0000 scale=1.0000 norm=0.4077\n",
      "[iter 2500] loss=-3.8666 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-3.8798 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2700] loss=-3.8805 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2800] loss=-3.8802 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-3.8912 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-3.9090 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 3100] loss=-3.9126 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3200] loss=-3.9178 val_loss=0.0000 scale=0.0078 norm=0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:16:02,044] Trial 1 finished with value: 7.401126650005833e-05 and parameters: {'n_estimators': 3232, 'learning_rate': 0.3814536680069899}. Best is trial 0 with value: 7.395342471927014e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0293 val_loss=0.0000 scale=1.0000 norm=0.4029\n",
      "[iter 200] loss=-3.3050 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.3613 val_loss=0.0000 scale=0.5000 norm=0.2025\n",
      "[iter 400] loss=-3.4596 val_loss=0.0000 scale=0.0625 norm=0.0253\n",
      "[iter 500] loss=-3.5693 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.5765 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.6627 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.6676 val_loss=0.0000 scale=1.0000 norm=0.4162\n",
      "[iter 900] loss=-3.7605 val_loss=0.0000 scale=1.0000 norm=0.4130\n",
      "[iter 1000] loss=-3.7696 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7729 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7787 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7800 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7841 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8036 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8036 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.8165 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8221 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-3.8388 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8387 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:16:40,773] Trial 2 finished with value: 9.697418690403428e-05 and parameters: {'n_estimators': 2017, 'learning_rate': 0.5135285703755009}. Best is trial 0 with value: 7.395342471927014e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2927 val_loss=0.0000 scale=1.0000 norm=0.4147\n",
      "[iter 200] loss=-3.4633 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.6146 val_loss=0.0000 scale=2.0000 norm=0.8216\n",
      "[iter 400] loss=-3.6609 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.7008 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7128 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 700] loss=-3.7268 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7877 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7873 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7875 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7897 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.7963 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1300] loss=-3.7993 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7989 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8558 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8950 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.9003 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8995 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9162 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.9152 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.9359 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9349 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.9377 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.9388 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-3.9590 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-3.9595 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2700] loss=-3.9656 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-3.9809 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-3.9896 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-4.0131 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3100] loss=-4.0254 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3200] loss=-4.0265 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3300] loss=-4.0278 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3400] loss=-4.0445 val_loss=0.0000 scale=0.0312 norm=0.0127\n",
      "[iter 3500] loss=-4.0654 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3600] loss=-4.0784 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3700] loss=-4.0777 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3800] loss=-4.0763 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3900] loss=-4.0944 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 4000] loss=-4.1045 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:17:59,448] Trial 3 finished with value: 7.062956490646486e-05 and parameters: {'n_estimators': 4094, 'learning_rate': 0.5356509495814795}. Best is trial 3 with value: 7.062956490646486e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1815 val_loss=0.0000 scale=1.0000 norm=0.4195\n",
      "[iter 200] loss=-3.2969 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 300] loss=-3.2967 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.3955 val_loss=0.0000 scale=1.0000 norm=0.4217\n",
      "[iter 500] loss=-3.4002 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 600] loss=-3.4392 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.4586 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 800] loss=-3.4651 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 900] loss=-3.5331 val_loss=0.0000 scale=1.0000 norm=0.4187\n",
      "[iter 1000] loss=-3.6072 val_loss=0.0000 scale=0.2500 norm=0.1060\n",
      "[iter 1100] loss=-3.7121 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7120 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7339 val_loss=0.0000 scale=0.5000 norm=0.2072\n",
      "[iter 1400] loss=-3.7606 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7777 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8396 val_loss=0.0000 scale=0.0625 norm=0.0255\n",
      "[iter 1700] loss=-3.8550 val_loss=0.0000 scale=0.0312 norm=0.0127\n",
      "[iter 1800] loss=-3.8546 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-3.8688 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2000] loss=-3.8702 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.8779 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.8884 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.9106 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-3.9257 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-3.9254 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-3.9256 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2700] loss=-3.9274 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-3.9289 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2900] loss=-3.9305 val_loss=0.0000 scale=1.0000 norm=0.4028\n",
      "[iter 3000] loss=-3.9311 val_loss=0.0000 scale=0.0078 norm=0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:18:53,014] Trial 4 finished with value: 8.014500501081087e-05 and parameters: {'n_estimators': 3059, 'learning_rate': 0.33811329174679705}. Best is trial 3 with value: 7.062956490646486e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0907 val_loss=0.0000 scale=1.0000 norm=0.4144\n",
      "[iter 200] loss=-3.3697 val_loss=0.0000 scale=1.0000 norm=0.4131\n",
      "[iter 300] loss=-3.4541 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.5061 val_loss=0.0000 scale=0.2500 norm=0.1032\n",
      "[iter 500] loss=-3.6081 val_loss=0.0000 scale=0.2500 norm=0.1020\n",
      "[iter 600] loss=-3.6553 val_loss=0.0000 scale=1.0000 norm=0.4107\n",
      "[iter 700] loss=-3.6743 val_loss=0.0000 scale=0.5000 norm=0.2037\n",
      "[iter 800] loss=-3.7158 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7161 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7181 val_loss=0.0000 scale=1.0000 norm=0.4060\n",
      "[iter 1100] loss=-3.7383 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7553 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7735 val_loss=0.0000 scale=1.0000 norm=0.4042\n",
      "[iter 1400] loss=-3.8112 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-3.8129 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-3.8290 val_loss=0.0000 scale=0.0156 norm=0.0064\n",
      "[iter 1700] loss=-3.8622 val_loss=0.0000 scale=0.1250 norm=0.0507\n",
      "[iter 1800] loss=-3.8831 val_loss=0.0000 scale=0.5000 norm=0.2040\n",
      "[iter 1900] loss=-3.8922 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2000] loss=-3.8944 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2100] loss=-3.9071 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9128 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.9130 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.9369 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2500] loss=-3.9401 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-3.9470 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:19:35,975] Trial 5 finished with value: 6.810173814562518e-05 and parameters: {'n_estimators': 2640, 'learning_rate': 0.3069502732772381}. Best is trial 5 with value: 6.810173814562518e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2576 val_loss=0.0000 scale=0.5000 norm=0.2058\n",
      "[iter 200] loss=-3.4721 val_loss=0.0000 scale=0.5000 norm=0.2077\n",
      "[iter 300] loss=-3.6225 val_loss=0.0000 scale=0.5000 norm=0.2076\n",
      "[iter 400] loss=-3.6309 val_loss=0.0000 scale=0.2500 norm=0.1043\n",
      "[iter 500] loss=-3.6887 val_loss=0.0000 scale=0.5000 norm=0.2084\n",
      "[iter 600] loss=-3.7164 val_loss=0.0000 scale=1.0000 norm=0.4154\n",
      "[iter 700] loss=-3.7374 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7704 val_loss=0.0000 scale=2.0000 norm=0.8270\n",
      "[iter 900] loss=-3.7938 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7936 val_loss=0.0000 scale=0.2500 norm=0.1034\n",
      "[iter 1100] loss=-3.8208 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.8262 val_loss=0.0000 scale=0.1250 norm=0.0519\n",
      "[iter 1300] loss=-3.8282 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.8310 val_loss=0.0000 scale=0.5000 norm=0.2072\n",
      "[iter 1500] loss=-3.8323 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8338 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.8600 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1800] loss=-3.8702 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.8715 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8807 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.8810 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2200] loss=-3.8806 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:20:12,346] Trial 6 finished with value: 8.769932545011703e-05 and parameters: {'n_estimators': 2210, 'learning_rate': 0.3257341856627446}. Best is trial 5 with value: 6.810173814562518e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0930 val_loss=0.0000 scale=0.2500 norm=0.1008\n",
      "[iter 200] loss=-3.3425 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.3958 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.3945 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.4305 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.5437 val_loss=0.0000 scale=0.5000 norm=0.2064\n",
      "[iter 700] loss=-3.5635 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.5785 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.5815 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.5797 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6137 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:20:31,665] Trial 7 finished with value: 0.00011395723550100156 and parameters: {'n_estimators': 1105, 'learning_rate': 0.4834564275797286}. Best is trial 5 with value: 6.810173814562518e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0012 val_loss=0.0000 scale=0.0625 norm=0.0257\n",
      "[iter 200] loss=-3.3440 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.4286 val_loss=0.0000 scale=0.5000 norm=0.2018\n",
      "[iter 400] loss=-3.5261 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.5248 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 600] loss=-3.5216 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.5425 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.5478 val_loss=0.0000 scale=0.5000 norm=0.2001\n",
      "[iter 900] loss=-3.6292 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6394 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6455 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.6719 val_loss=0.0000 scale=0.2500 norm=0.1023\n",
      "[iter 1300] loss=-3.6930 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7219 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7200 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.7304 val_loss=0.0000 scale=0.2500 norm=0.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:20:59,915] Trial 8 finished with value: 0.00011681598826677196 and parameters: {'n_estimators': 1633, 'learning_rate': 0.5804168854698942}. Best is trial 5 with value: 6.810173814562518e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0303 val_loss=0.0000 scale=0.5000 norm=0.2148\n",
      "[iter 200] loss=-3.2503 val_loss=0.0000 scale=1.0000 norm=0.4299\n",
      "[iter 300] loss=-3.3219 val_loss=0.0000 scale=0.2500 norm=0.1052\n",
      "[iter 400] loss=-3.3797 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.4664 val_loss=0.0000 scale=1.0000 norm=0.4182\n",
      "[iter 600] loss=-3.4851 val_loss=0.0000 scale=1.0000 norm=0.4164\n",
      "[iter 700] loss=-3.4890 val_loss=0.0000 scale=0.0156 norm=0.0065\n",
      "[iter 800] loss=-3.4894 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 900] loss=-3.4898 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.4897 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1100] loss=-3.4933 val_loss=0.0000 scale=0.5000 norm=0.2081\n",
      "[iter 1200] loss=-3.4993 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.4995 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.5104 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.5104 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.5104 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1700] loss=-3.5392 val_loss=0.0000 scale=1.0000 norm=0.4145\n",
      "[iter 1800] loss=-3.5418 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.5429 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.5557 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.5556 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.5899 val_loss=0.0000 scale=0.5000 norm=0.2036\n",
      "[iter 2300] loss=-3.6049 val_loss=0.0000 scale=0.2500 norm=0.1022\n",
      "[iter 2400] loss=-3.6281 val_loss=0.0000 scale=1.0000 norm=0.4103\n",
      "[iter 2500] loss=-3.6361 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-3.6444 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 2700] loss=-3.6445 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 2800] loss=-3.6444 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 2900] loss=-3.6453 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 3000] loss=-3.6618 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3100] loss=-3.6620 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3200] loss=-3.6652 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3300] loss=-3.6657 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 3400] loss=-3.6776 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3500] loss=-3.6826 val_loss=0.0000 scale=0.5000 norm=0.2071\n",
      "[iter 3600] loss=-3.7096 val_loss=0.0000 scale=0.2500 norm=0.1023\n",
      "[iter 3700] loss=-3.7196 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3800] loss=-3.7365 val_loss=0.0000 scale=1.0000 norm=0.4072\n",
      "[iter 3900] loss=-3.7686 val_loss=0.0000 scale=0.5000 norm=0.2040\n",
      "[iter 4000] loss=-3.7795 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4100] loss=-3.7814 val_loss=0.0000 scale=0.1250 norm=0.0508\n",
      "[iter 4200] loss=-3.7884 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 4300] loss=-3.7923 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4400] loss=-3.7923 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:22:17,375] Trial 9 finished with value: 8.250505286108448e-05 and parameters: {'n_estimators': 4416, 'learning_rate': 0.12794446992945516}. Best is trial 5 with value: 6.810173814562518e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.9571 val_loss=0.0000 scale=0.5000 norm=0.2161\n",
      "[iter 200] loss=-3.0089 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.3173 val_loss=0.0000 scale=1.0000 norm=0.4284\n",
      "[iter 400] loss=-3.4559 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 500] loss=-3.5408 val_loss=0.0000 scale=1.0000 norm=0.4164\n",
      "[iter 600] loss=-3.6120 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.6165 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.6559 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7030 val_loss=0.0000 scale=0.2500 norm=0.1020\n",
      "[iter 1000] loss=-3.7143 val_loss=0.0000 scale=0.0156 norm=0.0064\n",
      "[iter 1100] loss=-3.7588 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-3.7748 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7802 val_loss=0.0000 scale=1.0000 norm=0.3991\n",
      "[iter 1400] loss=-3.8182 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8308 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8404 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1700] loss=-3.8591 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8606 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-3.8774 val_loss=0.0000 scale=0.5000 norm=0.1978\n",
      "[iter 2000] loss=-3.8917 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2100] loss=-3.8971 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-3.9004 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-3.9036 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-3.9064 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2500] loss=-3.9101 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-3.9263 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2700] loss=-3.9401 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-3.9419 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-3.9453 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-3.9554 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-3.9581 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-3.9687 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3300] loss=-3.9734 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-3.9722 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3500] loss=-3.9778 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3600] loss=-3.9814 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3700] loss=-3.9901 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3800] loss=-3.9944 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 3900] loss=-4.0022 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:23:25,278] Trial 10 finished with value: 9.58004777620474e-05 and parameters: {'n_estimators': 3988, 'learning_rate': 0.7903464022206964}. Best is trial 5 with value: 6.810173814562518e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.3768 val_loss=0.0000 scale=2.0000 norm=0.7830\n",
      "[iter 200] loss=-2.9348 val_loss=0.0000 scale=1.0000 norm=0.4016\n",
      "[iter 300] loss=-3.0703 val_loss=0.0000 scale=1.0000 norm=0.4162\n",
      "[iter 400] loss=-3.1537 val_loss=0.0000 scale=1.0000 norm=0.4096\n",
      "[iter 500] loss=-3.2275 val_loss=0.0000 scale=1.0000 norm=0.4059\n",
      "[iter 600] loss=-3.2764 val_loss=0.0000 scale=0.2500 norm=0.1018\n",
      "[iter 700] loss=-3.3155 val_loss=0.0000 scale=1.0000 norm=0.4081\n",
      "[iter 800] loss=-3.3497 val_loss=0.0000 scale=0.2500 norm=0.1024\n",
      "[iter 900] loss=-3.3745 val_loss=0.0000 scale=0.2500 norm=0.1025\n",
      "[iter 1000] loss=-3.3917 val_loss=0.0000 scale=1.0000 norm=0.4080\n",
      "[iter 1100] loss=-3.4111 val_loss=0.0000 scale=1.0000 norm=0.4063\n",
      "[iter 1200] loss=-3.4117 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1300] loss=-3.4121 val_loss=0.0000 scale=1.0000 norm=0.4070\n",
      "[iter 1400] loss=-3.4327 val_loss=0.0000 scale=0.2500 norm=0.1017\n",
      "[iter 1500] loss=-3.4456 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1600] loss=-3.4456 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1700] loss=-3.4477 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.4480 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.4479 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.4485 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.4577 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.4608 val_loss=0.0000 scale=0.5000 norm=0.2055\n",
      "[iter 2300] loss=-3.4611 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.4611 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-3.4615 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-3.4782 val_loss=0.0000 scale=0.5000 norm=0.2050\n",
      "[iter 2700] loss=-3.4822 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-3.4822 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-3.4839 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-3.4840 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3100] loss=-3.4859 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3200] loss=-3.4864 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 3300] loss=-3.4896 val_loss=0.0000 scale=0.5000 norm=0.2063\n",
      "[iter 3400] loss=-3.4932 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 3500] loss=-3.4932 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 3600] loss=-3.4936 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 3700] loss=-3.4939 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 3800] loss=-3.4979 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 3900] loss=-3.4978 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 4000] loss=-3.4978 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 4100] loss=-3.5013 val_loss=0.0000 scale=0.5000 norm=0.2066\n",
      "[iter 4200] loss=-3.5039 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 4300] loss=-3.5060 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 4400] loss=-3.5060 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 4500] loss=-3.5064 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 4600] loss=-3.5072 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 4700] loss=-3.5073 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 4800] loss=-3.5125 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 4900] loss=-3.5130 val_loss=0.0000 scale=0.0020 norm=0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:24:50,658] Trial 11 finished with value: 0.00011696648374806582 and parameters: {'n_estimators': 4903, 'learning_rate': 0.03141340683305982}. Best is trial 5 with value: 6.810173814562518e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.9751 val_loss=0.0000 scale=1.0000 norm=0.4383\n",
      "[iter 200] loss=-3.3781 val_loss=0.0000 scale=1.0000 norm=0.4245\n",
      "[iter 300] loss=-3.6143 val_loss=0.0000 scale=1.0000 norm=0.4136\n",
      "[iter 400] loss=-3.7491 val_loss=0.0000 scale=1.0000 norm=0.4048\n",
      "[iter 500] loss=-3.7768 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7907 val_loss=0.0000 scale=0.1250 norm=0.0502\n",
      "[iter 700] loss=-3.8201 val_loss=0.0000 scale=1.0000 norm=0.3926\n",
      "[iter 800] loss=-3.8347 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 900] loss=-3.9016 val_loss=0.0000 scale=1.0000 norm=0.3966\n",
      "[iter 1000] loss=-3.9528 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-3.9801 val_loss=0.0000 scale=0.5000 norm=0.1955\n",
      "[iter 1200] loss=-4.0123 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0484 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1400] loss=-4.0648 val_loss=0.0000 scale=1.0000 norm=0.3823\n",
      "[iter 1500] loss=-4.0732 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 1600] loss=-4.0929 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1700] loss=-4.1045 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 1800] loss=-4.1254 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1900] loss=-4.1352 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2000] loss=-4.1358 val_loss=0.0000 scale=0.5000 norm=0.1865\n",
      "[iter 2100] loss=-4.1539 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 2200] loss=-4.1629 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 2300] loss=-4.1796 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2400] loss=-4.1833 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 2500] loss=-4.2035 val_loss=0.0000 scale=0.5000 norm=0.1861\n",
      "[iter 2600] loss=-4.2125 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2700] loss=-4.2169 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 2800] loss=-4.2332 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 2900] loss=-4.2427 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 3000] loss=-4.2500 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 3100] loss=-4.2527 val_loss=0.0000 scale=0.0039 norm=0.0014\n",
      "[iter 3200] loss=-4.2687 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 3300] loss=-4.2775 val_loss=0.0000 scale=0.1250 norm=0.0461\n",
      "[iter 3400] loss=-4.2784 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 3500] loss=-4.2804 val_loss=0.0000 scale=0.0039 norm=0.0014\n",
      "[iter 3600] loss=-4.2900 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 3700] loss=-4.2915 val_loss=0.0000 scale=2.0000 norm=0.7359\n",
      "[iter 3800] loss=-4.3045 val_loss=0.0000 scale=0.0078 norm=0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:26:19,451] Trial 12 finished with value: 6.609412884416653e-05 and parameters: {'n_estimators': 3806, 'learning_rate': 0.9674802821958264}. Best is trial 12 with value: 6.609412884416653e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.9505 val_loss=0.0000 scale=0.5000 norm=0.2172\n",
      "[iter 200] loss=-3.2165 val_loss=0.0000 scale=1.0000 norm=0.4264\n",
      "[iter 300] loss=-3.4641 val_loss=0.0000 scale=1.0000 norm=0.4284\n",
      "[iter 400] loss=-3.5380 val_loss=0.0000 scale=1.0000 norm=0.4278\n",
      "[iter 500] loss=-3.6307 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 600] loss=-3.6559 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7487 val_loss=0.0000 scale=1.0000 norm=0.4160\n",
      "[iter 800] loss=-3.8342 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 900] loss=-3.8747 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.8983 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9166 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.9474 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-3.9455 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.9421 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.9914 val_loss=0.0000 scale=1.0000 norm=0.4104\n",
      "[iter 1600] loss=-4.0184 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-4.0175 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1800] loss=-4.0590 val_loss=0.0000 scale=0.2500 norm=0.1012\n",
      "[iter 1900] loss=-4.0720 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.0790 val_loss=0.0000 scale=0.2500 norm=0.1003\n",
      "[iter 2100] loss=-4.0812 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.0854 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.0915 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1148 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.1336 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.1422 val_loss=0.0000 scale=0.0312 norm=0.0125\n",
      "[iter 2700] loss=-4.1691 val_loss=0.0000 scale=0.5000 norm=0.1999\n",
      "[iter 2800] loss=-4.1884 val_loss=0.0000 scale=0.5000 norm=0.1981\n",
      "[iter 2900] loss=-4.1955 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.1970 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.2073 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3200] loss=-4.2130 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3300] loss=-4.2186 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.2255 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3500] loss=-4.2422 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:27:47,027] Trial 13 finished with value: 7.332479981955927e-05 and parameters: {'n_estimators': 3550, 'learning_rate': 0.9613778438919263}. Best is trial 12 with value: 6.609412884416653e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0580 val_loss=0.0000 scale=1.0000 norm=0.4540\n",
      "[iter 200] loss=-3.4175 val_loss=0.0000 scale=0.5000 norm=0.2146\n",
      "[iter 300] loss=-3.4464 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 400] loss=-3.4455 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.4730 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 600] loss=-3.5654 val_loss=0.0000 scale=1.0000 norm=0.4270\n",
      "[iter 700] loss=-3.5957 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.6581 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7050 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7459 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.8099 val_loss=0.0000 scale=1.0000 norm=0.4087\n",
      "[iter 1200] loss=-3.8374 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.8367 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-3.8710 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8690 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.9267 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.9238 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.9375 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9654 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.9726 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-3.9931 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-3.9996 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-4.0014 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2400] loss=-4.0147 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 2500] loss=-4.0130 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 2600] loss=-4.0115 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:28:50,371] Trial 14 finished with value: 7.418337890152557e-05 and parameters: {'n_estimators': 2634, 'learning_rate': 0.9868246047096789}. Best is trial 12 with value: 6.609412884416653e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1279 val_loss=0.0000 scale=1.0000 norm=0.4354\n",
      "[iter 200] loss=-3.4838 val_loss=0.0000 scale=0.5000 norm=0.2143\n",
      "[iter 300] loss=-3.6772 val_loss=0.0000 scale=1.0000 norm=0.4240\n",
      "[iter 400] loss=-3.7625 val_loss=0.0000 scale=0.5000 norm=0.2108\n",
      "[iter 500] loss=-3.8319 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.8315 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.8903 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.8917 val_loss=0.0000 scale=0.5000 norm=0.2047\n",
      "[iter 900] loss=-3.9554 val_loss=0.0000 scale=0.5000 norm=0.2023\n",
      "[iter 1000] loss=-3.9960 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-4.0233 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-4.0549 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0999 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.1297 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.1467 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.1476 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.1518 val_loss=0.0000 scale=0.5000 norm=0.1983\n",
      "[iter 1800] loss=-4.1543 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.1574 val_loss=0.0000 scale=0.1250 norm=0.0493\n",
      "[iter 2000] loss=-4.1716 val_loss=0.0000 scale=0.5000 norm=0.1969\n",
      "[iter 2100] loss=-4.1834 val_loss=0.0000 scale=0.5000 norm=0.1954\n",
      "[iter 2200] loss=-4.1994 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.2134 val_loss=0.0000 scale=0.2500 norm=0.0984\n",
      "[iter 2400] loss=-4.2255 val_loss=0.0000 scale=0.1250 norm=0.0492\n",
      "[iter 2500] loss=-4.2363 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:29:48,198] Trial 15 finished with value: 5.798311769566367e-05 and parameters: {'n_estimators': 2534, 'learning_rate': 0.7327591366253451}. Best is trial 15 with value: 5.798311769566367e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.9941 val_loss=0.0000 scale=1.0000 norm=0.4351\n",
      "[iter 200] loss=-3.3374 val_loss=0.0000 scale=1.0000 norm=0.4155\n",
      "[iter 300] loss=-3.3928 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.5090 val_loss=0.0000 scale=1.0000 norm=0.4175\n",
      "[iter 500] loss=-3.5751 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.5891 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.5930 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.6455 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.6457 val_loss=0.0000 scale=1.0000 norm=0.4025\n",
      "[iter 1000] loss=-3.6673 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-3.7071 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7356 val_loss=0.0000 scale=0.0312 norm=0.0124\n",
      "[iter 1300] loss=-3.7387 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7588 val_loss=0.0000 scale=0.0625 norm=0.0248\n",
      "[iter 1500] loss=-3.7692 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-3.7708 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.7794 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-3.8090 val_loss=0.0000 scale=0.5000 norm=0.1984\n",
      "[iter 1900] loss=-3.8341 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8741 val_loss=0.0000 scale=1.0000 norm=0.3901\n",
      "[iter 2100] loss=-3.8868 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2200] loss=-3.9098 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-3.9166 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2400] loss=-3.9307 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-3.9367 val_loss=0.0000 scale=1.0000 norm=0.3849\n",
      "[iter 2600] loss=-3.9652 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-3.9784 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-3.9816 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.0011 val_loss=0.0000 scale=0.5000 norm=0.1942\n",
      "[iter 3000] loss=-4.0079 val_loss=0.0000 scale=0.2500 norm=0.0970\n",
      "[iter 3100] loss=-4.0103 val_loss=0.0000 scale=2.0000 norm=0.7740\n",
      "[iter 3200] loss=-4.0126 val_loss=0.0000 scale=0.1250 norm=0.0484\n",
      "[iter 3300] loss=-4.0229 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.0245 val_loss=0.0000 scale=1.0000 norm=0.3872\n",
      "[iter 3500] loss=-4.0311 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:31:05,046] Trial 16 finished with value: 9.030241907642531e-05 and parameters: {'n_estimators': 3562, 'learning_rate': 0.8052399134849538}. Best is trial 15 with value: 5.798311769566367e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0140 val_loss=0.0000 scale=0.5000 norm=0.2126\n",
      "[iter 200] loss=-3.3144 val_loss=0.0000 scale=1.0000 norm=0.4085\n",
      "[iter 300] loss=-3.3978 val_loss=0.0000 scale=0.5000 norm=0.2034\n",
      "[iter 400] loss=-3.4612 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.4863 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.4870 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.5212 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.5547 val_loss=0.0000 scale=1.0000 norm=0.4004\n",
      "[iter 900] loss=-3.5826 val_loss=0.0000 scale=0.5000 norm=0.2009\n",
      "[iter 1000] loss=-3.6147 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6310 val_loss=0.0000 scale=0.5000 norm=0.2008\n",
      "[iter 1200] loss=-3.6350 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.6333 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.6837 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-3.6909 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.7099 val_loss=0.0000 scale=0.1250 norm=0.0502\n",
      "[iter 1700] loss=-3.7177 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.7687 val_loss=0.0000 scale=0.5000 norm=0.1983\n",
      "[iter 1900] loss=-3.8071 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8091 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-3.8259 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.8709 val_loss=0.0000 scale=0.5000 norm=0.1993\n",
      "[iter 2300] loss=-3.8908 val_loss=0.0000 scale=1.0000 norm=0.3966\n",
      "[iter 2400] loss=-3.9116 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-3.9517 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:31:46,091] Trial 17 finished with value: 9.06514603633175e-05 and parameters: {'n_estimators': 2510, 'learning_rate': 0.7889232729452278}. Best is trial 15 with value: 5.798311769566367e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2597 val_loss=0.0000 scale=1.0000 norm=0.4186\n",
      "[iter 200] loss=-3.4059 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.5425 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 400] loss=-3.6299 val_loss=0.0000 scale=1.0000 norm=0.4087\n",
      "[iter 500] loss=-3.7043 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7387 val_loss=0.0000 scale=1.0000 norm=0.3957\n",
      "[iter 700] loss=-3.7388 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 800] loss=-3.7667 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7731 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7872 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-3.7986 val_loss=0.0000 scale=1.0000 norm=0.3976\n",
      "[iter 1200] loss=-3.8444 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.8607 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.8849 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8883 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8877 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.9089 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.9059 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9057 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.9191 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.9271 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9299 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2300] loss=-3.9312 val_loss=0.0000 scale=0.5000 norm=0.1972\n",
      "[iter 2400] loss=-3.9554 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2500] loss=-3.9690 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-3.9895 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2700] loss=-4.0172 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.0187 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.0346 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.0375 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.0431 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.0514 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3300] loss=-4.0552 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.0724 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3500] loss=-4.0788 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3600] loss=-4.0954 val_loss=0.0000 scale=0.5000 norm=0.1972\n",
      "[iter 3700] loss=-4.1220 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3800] loss=-4.1251 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3900] loss=-4.1230 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4000] loss=-4.1278 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4100] loss=-4.1340 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4200] loss=-4.1372 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 4300] loss=-4.1430 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 4400] loss=-4.1444 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 4500] loss=-4.1506 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4600] loss=-4.1539 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:33:12,560] Trial 18 finished with value: 6.317209709351785e-05 and parameters: {'n_estimators': 4693, 'learning_rate': 0.6777678951188609}. Best is trial 15 with value: 5.798311769566367e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2185 val_loss=0.0000 scale=1.0000 norm=0.4241\n",
      "[iter 200] loss=-3.4650 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.5734 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 400] loss=-3.5765 val_loss=0.0000 scale=0.5000 norm=0.2070\n",
      "[iter 500] loss=-3.6866 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7094 val_loss=0.0000 scale=0.5000 norm=0.2034\n",
      "[iter 700] loss=-3.7406 val_loss=0.0000 scale=0.2500 norm=0.1004\n",
      "[iter 800] loss=-3.7655 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.8395 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1000] loss=-3.8648 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9142 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-3.9324 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1300] loss=-3.9474 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-3.9665 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-3.9763 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.9796 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.9931 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-4.0013 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.0232 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-4.0229 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.0451 val_loss=0.0000 scale=0.0625 norm=0.0248\n",
      "[iter 2200] loss=-4.0638 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2300] loss=-4.0663 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.0700 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.0838 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2600] loss=-4.0893 val_loss=0.0000 scale=0.2500 norm=0.0985\n",
      "[iter 2700] loss=-4.0939 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-4.1066 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.1159 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.1191 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.1248 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3200] loss=-4.1268 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3300] loss=-4.1287 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.1371 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3500] loss=-4.1366 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3600] loss=-4.1374 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3700] loss=-4.1457 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 3800] loss=-4.1500 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3900] loss=-4.1521 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4000] loss=-4.1596 val_loss=0.0000 scale=0.0312 norm=0.0123\n",
      "[iter 4100] loss=-4.1632 val_loss=0.0000 scale=0.0312 norm=0.0124\n",
      "[iter 4200] loss=-4.1627 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4300] loss=-4.1639 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4400] loss=-4.1788 val_loss=0.0000 scale=0.5000 norm=0.1969\n",
      "[iter 4500] loss=-4.1854 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 4600] loss=-4.1906 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 4700] loss=-4.1967 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4800] loss=-4.1971 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4900] loss=-4.2005 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:34:35,905] Trial 19 finished with value: 6.708761032230795e-05 and parameters: {'n_estimators': 4994, 'learning_rate': 0.6447891900315913}. Best is trial 15 with value: 5.798311769566367e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1829 val_loss=0.0000 scale=0.1250 norm=0.0518\n",
      "[iter 200] loss=-3.4736 val_loss=0.0000 scale=0.2500 norm=0.1045\n",
      "[iter 300] loss=-3.6230 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.6448 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.6840 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7080 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 700] loss=-3.7769 val_loss=0.0000 scale=0.5000 norm=0.2037\n",
      "[iter 800] loss=-3.7909 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.8028 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.8399 val_loss=0.0000 scale=1.0000 norm=0.4095\n",
      "[iter 1100] loss=-3.8683 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8670 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-3.8745 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.8790 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8803 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8843 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:35:04,175] Trial 20 finished with value: 9.644408774851229e-05 and parameters: {'n_estimators': 1650, 'learning_rate': 0.6853629288933394}. Best is trial 15 with value: 5.798311769566367e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0574 val_loss=0.0000 scale=0.0625 norm=0.0279\n",
      "[iter 200] loss=-3.0717 val_loss=0.0000 scale=0.0020 norm=0.0009\n",
      "[iter 300] loss=-3.1222 val_loss=0.0000 scale=0.5000 norm=0.2235\n",
      "[iter 400] loss=-3.4732 val_loss=0.0000 scale=1.0000 norm=0.4272\n",
      "[iter 500] loss=-3.6666 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6792 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7492 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7563 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7702 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.8109 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-3.8494 val_loss=0.0000 scale=1.0000 norm=0.4027\n",
      "[iter 1200] loss=-3.8714 val_loss=0.0000 scale=0.5000 norm=0.2007\n",
      "[iter 1300] loss=-3.8770 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-3.9120 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.9452 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1600] loss=-3.9605 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1700] loss=-3.9795 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-3.9901 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1900] loss=-4.0080 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2000] loss=-4.0255 val_loss=0.0000 scale=0.5000 norm=0.1957\n",
      "[iter 2100] loss=-4.0389 val_loss=0.0000 scale=1.0000 norm=0.3917\n",
      "[iter 2200] loss=-4.0454 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.0509 val_loss=0.0000 scale=0.2500 norm=0.0981\n",
      "[iter 2400] loss=-4.0709 val_loss=0.0000 scale=1.0000 norm=0.3890\n",
      "[iter 2500] loss=-4.0780 val_loss=0.0000 scale=1.0000 norm=0.3867\n",
      "[iter 2600] loss=-4.0866 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2700] loss=-4.0917 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-4.1011 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2900] loss=-4.1091 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 3000] loss=-4.1117 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.1210 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.1218 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3300] loss=-4.1445 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.1620 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3500] loss=-4.1675 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 3600] loss=-4.1745 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3700] loss=-4.1819 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3800] loss=-4.1839 val_loss=0.0000 scale=1.0000 norm=0.3850\n",
      "[iter 3900] loss=-4.1899 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4000] loss=-4.1929 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 4100] loss=-4.1941 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 4200] loss=-4.2085 val_loss=0.0000 scale=1.0000 norm=0.3795\n",
      "[iter 4300] loss=-4.2132 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4400] loss=-4.2182 val_loss=0.0000 scale=0.0312 norm=0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:36:17,525] Trial 21 finished with value: 8.095485437800454e-05 and parameters: {'n_estimators': 4471, 'learning_rate': 0.896012173931461}. Best is trial 15 with value: 5.798311769566367e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1009 val_loss=0.0000 scale=1.0000 norm=0.4459\n",
      "[iter 200] loss=-3.4861 val_loss=0.0000 scale=1.0000 norm=0.4257\n",
      "[iter 300] loss=-3.6913 val_loss=0.0000 scale=1.0000 norm=0.4233\n",
      "[iter 400] loss=-3.7531 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.8396 val_loss=0.0000 scale=0.5000 norm=0.2069\n",
      "[iter 600] loss=-3.8673 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.9101 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.9206 val_loss=0.0000 scale=0.5000 norm=0.2044\n",
      "[iter 900] loss=-3.9703 val_loss=0.0000 scale=0.5000 norm=0.2009\n",
      "[iter 1000] loss=-4.0111 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-4.0437 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-4.0537 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-4.0909 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-4.0988 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1500] loss=-4.1159 val_loss=0.0000 scale=1.0000 norm=0.3996\n",
      "[iter 1600] loss=-4.1315 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.1632 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1689 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.1794 val_loss=0.0000 scale=0.0625 norm=0.0249\n",
      "[iter 2000] loss=-4.1910 val_loss=0.0000 scale=1.0000 norm=0.3943\n",
      "[iter 2100] loss=-4.2171 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-4.2178 val_loss=0.0000 scale=1.0000 norm=0.3974\n",
      "[iter 2300] loss=-4.2265 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.2286 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2500] loss=-4.2392 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.2457 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.2646 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.2680 val_loss=0.0000 scale=1.0000 norm=0.3868\n",
      "[iter 2900] loss=-4.2767 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.2832 val_loss=0.0000 scale=1.0000 norm=0.3880\n",
      "[iter 3100] loss=-4.2981 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.3065 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3300] loss=-4.3128 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.3184 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3500] loss=-4.3217 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3600] loss=-4.3286 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3700] loss=-4.3287 val_loss=0.0000 scale=0.1250 norm=0.0481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:37:16,581] Trial 22 finished with value: 6.775996543405486e-05 and parameters: {'n_estimators': 3728, 'learning_rate': 0.8756614767155076}. Best is trial 15 with value: 5.798311769566367e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1294 val_loss=0.0000 scale=1.0000 norm=0.4206\n",
      "[iter 200] loss=-3.3817 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.5388 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.5628 val_loss=0.0000 scale=0.1250 norm=0.0531\n",
      "[iter 500] loss=-3.5660 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 600] loss=-3.5716 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 700] loss=-3.6816 val_loss=0.0000 scale=1.0000 norm=0.4190\n",
      "[iter 800] loss=-3.7043 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7145 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7140 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7269 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7541 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7613 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7710 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8051 val_loss=0.0000 scale=0.1250 norm=0.0518\n",
      "[iter 1600] loss=-3.8251 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.8318 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8505 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-3.8508 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8670 val_loss=0.0000 scale=0.2500 norm=0.1035\n",
      "[iter 2100] loss=-3.8710 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2200] loss=-3.8919 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2300] loss=-3.8967 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2400] loss=-3.9134 val_loss=0.0000 scale=0.2500 norm=0.1016\n",
      "[iter 2500] loss=-3.9155 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-3.9146 val_loss=0.0000 scale=0.0156 norm=0.0064\n",
      "[iter 2700] loss=-3.9334 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-3.9333 val_loss=0.0000 scale=0.2500 norm=0.1022\n",
      "[iter 2900] loss=-3.9411 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-3.9410 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3100] loss=-3.9817 val_loss=0.0000 scale=0.2500 norm=0.1023\n",
      "[iter 3200] loss=-3.9997 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3300] loss=-3.9988 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3400] loss=-4.0141 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3500] loss=-4.0208 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 3600] loss=-4.0291 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3700] loss=-4.0301 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3800] loss=-4.0575 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3900] loss=-4.0566 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 4000] loss=-4.0636 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 4100] loss=-4.0654 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4200] loss=-4.0654 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4300] loss=-4.0835 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4400] loss=-4.0847 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 4500] loss=-4.1106 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4600] loss=-4.1097 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:38:40,022] Trial 23 finished with value: 8.043245914900594e-05 and parameters: {'n_estimators': 4654, 'learning_rate': 0.7140108670588788}. Best is trial 15 with value: 5.798311769566367e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1627 val_loss=0.0000 scale=1.0000 norm=0.4446\n",
      "[iter 200] loss=-3.5806 val_loss=0.0000 scale=0.5000 norm=0.2203\n",
      "[iter 300] loss=-3.7972 val_loss=0.0000 scale=1.0000 norm=0.4235\n",
      "[iter 400] loss=-3.8713 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.9663 val_loss=0.0000 scale=0.5000 norm=0.2039\n",
      "[iter 600] loss=-3.9986 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-4.0064 val_loss=0.0000 scale=0.5000 norm=0.2048\n",
      "[iter 800] loss=-4.0582 val_loss=0.0000 scale=1.0000 norm=0.4022\n",
      "[iter 900] loss=-4.1056 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-4.1193 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1100] loss=-4.1386 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-4.1497 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.1601 val_loss=0.0000 scale=1.0000 norm=0.3987\n",
      "[iter 1400] loss=-4.1667 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.1910 val_loss=0.0000 scale=2.0000 norm=0.7847\n",
      "[iter 1600] loss=-4.2096 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1700] loss=-4.2123 val_loss=0.0000 scale=1.0000 norm=0.3897\n",
      "[iter 1800] loss=-4.2120 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.2328 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2000] loss=-4.2491 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2100] loss=-4.2519 val_loss=0.0000 scale=1.0000 norm=0.3861\n",
      "[iter 2200] loss=-4.2661 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2300] loss=-4.2715 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2400] loss=-4.2838 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-4.2853 val_loss=0.0000 scale=0.2500 norm=0.0953\n",
      "[iter 2600] loss=-4.2963 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.3044 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.3060 val_loss=0.0000 scale=0.0312 norm=0.0119\n",
      "[iter 2900] loss=-4.3062 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.3078 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.3215 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.3307 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3300] loss=-4.3405 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.3450 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3500] loss=-4.3531 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3600] loss=-4.3586 val_loss=0.0000 scale=0.5000 norm=0.1892\n",
      "[iter 3700] loss=-4.3613 val_loss=0.0000 scale=2.0000 norm=0.7592\n",
      "[iter 3800] loss=-4.3691 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3900] loss=-4.3747 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4000] loss=-4.3781 val_loss=0.0000 scale=0.1250 norm=0.0472\n",
      "[iter 4100] loss=-4.3877 val_loss=0.0000 scale=0.0078 norm=0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:39:58,657] Trial 24 finished with value: 5.592775553556088e-05 and parameters: {'n_estimators': 4140, 'learning_rate': 0.8854198851197161}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0993 val_loss=0.0000 scale=1.0000 norm=0.4441\n",
      "[iter 200] loss=-3.4747 val_loss=0.0000 scale=1.0000 norm=0.4343\n",
      "[iter 300] loss=-3.5397 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.5717 val_loss=0.0000 scale=0.5000 norm=0.2163\n",
      "[iter 500] loss=-3.7001 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 600] loss=-3.7659 val_loss=0.0000 scale=0.5000 norm=0.2106\n",
      "[iter 700] loss=-3.8541 val_loss=0.0000 scale=0.5000 norm=0.2076\n",
      "[iter 800] loss=-3.8954 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 900] loss=-3.9062 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.9395 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9464 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.9739 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-4.0000 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-4.0433 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-4.0479 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1600] loss=-4.0692 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.0767 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.0870 val_loss=0.0000 scale=0.1250 norm=0.0501\n",
      "[iter 1900] loss=-4.1017 val_loss=0.0000 scale=0.0625 norm=0.0251\n",
      "[iter 2000] loss=-4.1321 val_loss=0.0000 scale=0.5000 norm=0.1994\n",
      "[iter 2100] loss=-4.1417 val_loss=0.0000 scale=1.0000 norm=0.4007\n",
      "[iter 2200] loss=-4.1442 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2300] loss=-4.1538 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1609 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.1608 val_loss=0.0000 scale=0.5000 norm=0.1995\n",
      "[iter 2600] loss=-4.1776 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1833 val_loss=0.0000 scale=1.0000 norm=0.3982\n",
      "[iter 2800] loss=-4.1963 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2900] loss=-4.2023 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.2079 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.2122 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 3200] loss=-4.2202 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.2299 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 3400] loss=-4.2323 val_loss=0.0000 scale=2.0000 norm=0.7841\n",
      "[iter 3500] loss=-4.2390 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 3600] loss=-4.2395 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3700] loss=-4.2415 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3800] loss=-4.2461 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3900] loss=-4.2494 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4000] loss=-4.2560 val_loss=0.0000 scale=1.0000 norm=0.3929\n",
      "[iter 4100] loss=-4.2630 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 4200] loss=-4.2680 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:41:42,460] Trial 25 finished with value: 5.785978771443299e-05 and parameters: {'n_estimators': 4246, 'learning_rate': 0.7486790831114187}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2749 val_loss=0.0000 scale=1.0000 norm=0.4430\n",
      "[iter 200] loss=-3.6344 val_loss=0.0000 scale=1.0000 norm=0.4288\n",
      "[iter 300] loss=-3.8072 val_loss=0.0000 scale=0.1250 norm=0.0522\n",
      "[iter 400] loss=-3.8761 val_loss=0.0000 scale=0.5000 norm=0.2058\n",
      "[iter 500] loss=-3.9181 val_loss=0.0000 scale=0.5000 norm=0.2051\n",
      "[iter 600] loss=-3.9484 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.9479 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.9771 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-4.0075 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1000] loss=-4.0346 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-4.0479 val_loss=0.0000 scale=1.0000 norm=0.4028\n",
      "[iter 1200] loss=-4.1022 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.1179 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-4.1230 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-4.1302 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.1427 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-4.1428 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1475 val_loss=0.0000 scale=0.2500 norm=0.0994\n",
      "[iter 1900] loss=-4.1553 val_loss=0.0000 scale=0.5000 norm=0.1971\n",
      "[iter 2000] loss=-4.1742 val_loss=0.0000 scale=0.2500 norm=0.0988\n",
      "[iter 2100] loss=-4.1834 val_loss=0.0000 scale=0.0625 norm=0.0244\n",
      "[iter 2200] loss=-4.1963 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1989 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2400] loss=-4.2164 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-4.2349 val_loss=0.0000 scale=0.2500 norm=0.0967\n",
      "[iter 2600] loss=-4.2362 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.2459 val_loss=0.0000 scale=1.0000 norm=0.3881\n",
      "[iter 2800] loss=-4.2494 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.2509 val_loss=0.0000 scale=0.0312 norm=0.0122\n",
      "[iter 3000] loss=-4.2589 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.2595 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.2607 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.2629 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.2665 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3500] loss=-4.2672 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3600] loss=-4.2686 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 3700] loss=-4.2757 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3800] loss=-4.2840 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3900] loss=-4.2837 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4000] loss=-4.2900 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4100] loss=-4.2957 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4200] loss=-4.3020 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:43:28,090] Trial 26 finished with value: 6.956209241415398e-05 and parameters: {'n_estimators': 4220, 'learning_rate': 0.8720074451734068}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1644 val_loss=0.0000 scale=1.0000 norm=0.4273\n",
      "[iter 200] loss=-3.5756 val_loss=0.0000 scale=1.0000 norm=0.4205\n",
      "[iter 300] loss=-3.6319 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.6433 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.7004 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.8292 val_loss=0.0000 scale=0.5000 norm=0.2067\n",
      "[iter 700] loss=-3.9117 val_loss=0.0000 scale=1.0000 norm=0.4106\n",
      "[iter 800] loss=-4.0221 val_loss=0.0000 scale=0.2500 norm=0.0993\n",
      "[iter 900] loss=-4.0933 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-4.1165 val_loss=0.0000 scale=0.0625 norm=0.0248\n",
      "[iter 1100] loss=-4.1368 val_loss=0.0000 scale=0.5000 norm=0.1980\n",
      "[iter 1200] loss=-4.1521 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.1585 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.1663 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.1818 val_loss=0.0000 scale=1.0000 norm=0.3926\n",
      "[iter 1600] loss=-4.1986 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 1700] loss=-4.2219 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 1800] loss=-4.2329 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 1900] loss=-4.2386 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2000] loss=-4.2470 val_loss=0.0000 scale=1.0000 norm=0.3875\n",
      "[iter 2100] loss=-4.2562 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2200] loss=-4.2624 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2300] loss=-4.2674 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2400] loss=-4.2706 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-4.2826 val_loss=0.0000 scale=0.0312 norm=0.0120\n",
      "[iter 2600] loss=-4.2890 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.2901 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.2910 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.3009 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 3000] loss=-4.3171 val_loss=0.0000 scale=1.0000 norm=0.3809\n",
      "[iter 3100] loss=-4.3254 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 3200] loss=-4.3406 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3300] loss=-4.3395 val_loss=0.0000 scale=0.0156 norm=0.0059\n",
      "[iter 3400] loss=-4.3437 val_loss=0.0000 scale=0.0078 norm=0.0029\n",
      "[iter 3500] loss=-4.3497 val_loss=0.0000 scale=0.0156 norm=0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:44:24,348] Trial 27 finished with value: 6.938438279015333e-05 and parameters: {'n_estimators': 3526, 'learning_rate': 0.7737563587641787}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2459 val_loss=0.0000 scale=1.0000 norm=0.4402\n",
      "[iter 200] loss=-3.5247 val_loss=0.0000 scale=1.0000 norm=0.4290\n",
      "[iter 300] loss=-3.6564 val_loss=0.0000 scale=1.0000 norm=0.4231\n",
      "[iter 400] loss=-3.8008 val_loss=0.0000 scale=1.0000 norm=0.4203\n",
      "[iter 500] loss=-3.8679 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 600] loss=-3.8960 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 700] loss=-3.9529 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.9910 val_loss=0.0000 scale=0.5000 norm=0.2027\n",
      "[iter 900] loss=-4.0194 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-4.0280 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1100] loss=-4.0491 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-4.0738 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0778 val_loss=0.0000 scale=0.5000 norm=0.2019\n",
      "[iter 1400] loss=-4.0939 val_loss=0.0000 scale=0.5000 norm=0.2016\n",
      "[iter 1500] loss=-4.1061 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-4.1172 val_loss=0.0000 scale=0.5000 norm=0.2018\n",
      "[iter 1700] loss=-4.1273 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-4.1291 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.1436 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1492 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-4.1567 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.1602 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1611 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1728 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.1748 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.1885 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1910 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2800] loss=-4.1987 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2900] loss=-4.2188 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.2226 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.2261 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.2309 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3300] loss=-4.2354 val_loss=0.0000 scale=0.0156 norm=0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:45:16,563] Trial 28 finished with value: 5.981040608108265e-05 and parameters: {'n_estimators': 3308, 'learning_rate': 0.7342792552269599}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0395 val_loss=0.0000 scale=1.0000 norm=0.4204\n",
      "[iter 200] loss=-3.2844 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.3255 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.3890 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.4293 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.4827 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 700] loss=-3.5202 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 800] loss=-3.5275 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 900] loss=-3.5373 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.5389 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1100] loss=-3.5401 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.5520 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.5791 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1400] loss=-3.6151 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-3.6295 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-3.6362 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.6784 val_loss=0.0000 scale=0.1250 norm=0.0494\n",
      "[iter 1800] loss=-3.6939 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1900] loss=-3.7122 val_loss=0.0000 scale=0.5000 norm=0.1969\n",
      "[iter 2000] loss=-3.7165 val_loss=0.0000 scale=0.0039 norm=0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:45:51,637] Trial 29 finished with value: 0.00014545599905809885 and parameters: {'n_estimators': 2088, 'learning_rate': 0.6205288896246329}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2209 val_loss=0.0000 scale=0.5000 norm=0.2209\n",
      "[iter 200] loss=-3.4600 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 300] loss=-3.5305 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.6768 val_loss=0.0000 scale=1.0000 norm=0.4198\n",
      "[iter 500] loss=-3.7463 val_loss=0.0000 scale=1.0000 norm=0.4116\n",
      "[iter 600] loss=-3.7718 val_loss=0.0000 scale=1.0000 norm=0.4142\n",
      "[iter 700] loss=-3.8132 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.8305 val_loss=0.0000 scale=0.0156 norm=0.0064\n",
      "[iter 900] loss=-3.8726 val_loss=0.0000 scale=1.0000 norm=0.4139\n",
      "[iter 1000] loss=-3.9165 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9306 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.9545 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.9558 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-3.9763 val_loss=0.0000 scale=0.1250 norm=0.0511\n",
      "[iter 1500] loss=-3.9920 val_loss=0.0000 scale=0.5000 norm=0.2034\n",
      "[iter 1600] loss=-4.0079 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-4.0155 val_loss=0.0000 scale=1.0000 norm=0.4047\n",
      "[iter 1800] loss=-4.0391 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-4.0463 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 2000] loss=-4.0572 val_loss=0.0000 scale=1.0000 norm=0.4038\n",
      "[iter 2100] loss=-4.0585 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2200] loss=-4.0733 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2300] loss=-4.0757 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2400] loss=-4.0890 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 2500] loss=-4.0934 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.0960 val_loss=0.0000 scale=0.5000 norm=0.2011\n",
      "[iter 2700] loss=-4.1025 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:46:35,100] Trial 30 finished with value: 7.004606433190711e-05 and parameters: {'n_estimators': 2736, 'learning_rate': 0.7254639692236183}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2128 val_loss=0.0000 scale=1.0000 norm=0.4247\n",
      "[iter 200] loss=-3.5474 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 300] loss=-3.7119 val_loss=0.0000 scale=2.0000 norm=0.8412\n",
      "[iter 400] loss=-3.8222 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.8429 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.8734 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.8921 val_loss=0.0000 scale=1.0000 norm=0.4158\n",
      "[iter 800] loss=-3.9205 val_loss=0.0000 scale=0.5000 norm=0.2072\n",
      "[iter 900] loss=-3.9612 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.9973 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-4.0335 val_loss=0.0000 scale=2.0000 norm=0.8144\n",
      "[iter 1200] loss=-4.0365 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-4.0608 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-4.0943 val_loss=0.0000 scale=1.0000 norm=0.4019\n",
      "[iter 1500] loss=-4.1078 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.1233 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1700] loss=-4.1244 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1471 val_loss=0.0000 scale=0.0625 norm=0.0250\n",
      "[iter 1900] loss=-4.1578 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1676 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-4.1697 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.1699 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-4.1745 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1930 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2500] loss=-4.2163 val_loss=0.0000 scale=0.5000 norm=0.1983\n",
      "[iter 2600] loss=-4.2191 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.2252 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.2290 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2900] loss=-4.2380 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 3000] loss=-4.2462 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.2467 val_loss=0.0000 scale=1.0000 norm=0.3929\n",
      "[iter 3200] loss=-4.2604 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:47:26,641] Trial 31 finished with value: 5.8365450674146074e-05 and parameters: {'n_estimators': 3221, 'learning_rate': 0.7295742643436745}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1132 val_loss=0.0000 scale=1.0000 norm=0.4521\n",
      "[iter 200] loss=-3.5059 val_loss=0.0000 scale=1.0000 norm=0.4327\n",
      "[iter 300] loss=-3.6074 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.6850 val_loss=0.0000 scale=1.0000 norm=0.4215\n",
      "[iter 500] loss=-3.7355 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 600] loss=-3.7527 val_loss=0.0000 scale=0.5000 norm=0.2114\n",
      "[iter 700] loss=-3.7859 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 800] loss=-3.8179 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.8303 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.8445 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.8705 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.9003 val_loss=0.0000 scale=0.5000 norm=0.2059\n",
      "[iter 1300] loss=-3.9478 val_loss=0.0000 scale=0.5000 norm=0.2056\n",
      "[iter 1400] loss=-3.9701 val_loss=0.0000 scale=0.0625 norm=0.0256\n",
      "[iter 1500] loss=-3.9773 val_loss=0.0000 scale=0.2500 norm=0.1026\n",
      "[iter 1600] loss=-4.0032 val_loss=0.0000 scale=0.0312 norm=0.0126\n",
      "[iter 1700] loss=-4.0379 val_loss=0.0000 scale=0.0625 norm=0.0255\n",
      "[iter 1800] loss=-4.0539 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-4.0712 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.0741 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2100] loss=-4.0787 val_loss=0.0000 scale=1.0000 norm=0.4038\n",
      "[iter 2200] loss=-4.0943 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-4.0959 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.0955 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.0981 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.1070 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1170 val_loss=0.0000 scale=2.0000 norm=0.7984\n",
      "[iter 2800] loss=-4.1317 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2900] loss=-4.1477 val_loss=0.0000 scale=0.0156 norm=0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:48:13,090] Trial 32 finished with value: 7.812751999491267e-05 and parameters: {'n_estimators': 2947, 'learning_rate': 0.8304791254307372}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1348 val_loss=0.0000 scale=1.0000 norm=0.4363\n",
      "[iter 200] loss=-3.4936 val_loss=0.0000 scale=1.0000 norm=0.4244\n",
      "[iter 300] loss=-3.6746 val_loss=0.0000 scale=1.0000 norm=0.4208\n",
      "[iter 400] loss=-3.7184 val_loss=0.0000 scale=1.0000 norm=0.4172\n",
      "[iter 500] loss=-3.7839 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.8037 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.8317 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.8769 val_loss=0.0000 scale=0.2500 norm=0.1013\n",
      "[iter 900] loss=-3.9050 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1000] loss=-3.9182 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9422 val_loss=0.0000 scale=0.0625 norm=0.0248\n",
      "[iter 1200] loss=-3.9421 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-3.9562 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1400] loss=-3.9560 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.9912 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-4.0034 val_loss=0.0000 scale=0.1250 norm=0.0502\n",
      "[iter 1700] loss=-4.0089 val_loss=0.0000 scale=1.0000 norm=0.4016\n",
      "[iter 1800] loss=-4.0357 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1900] loss=-4.0415 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.0631 val_loss=0.0000 scale=1.0000 norm=0.3884\n",
      "[iter 2100] loss=-4.0638 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.0812 val_loss=0.0000 scale=1.0000 norm=0.3900\n",
      "[iter 2300] loss=-4.0785 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2400] loss=-4.0813 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-4.1099 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2600] loss=-4.1124 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.1298 val_loss=0.0000 scale=1.0000 norm=0.3873\n",
      "[iter 2800] loss=-4.1372 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.1365 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.1386 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.1460 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.1471 val_loss=0.0000 scale=1.0000 norm=0.3874\n",
      "[iter 3300] loss=-4.1518 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:49:07,694] Trial 33 finished with value: 7.960143394909034e-05 and parameters: {'n_estimators': 3303, 'learning_rate': 0.9193567874090562}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1114 val_loss=0.0000 scale=1.0000 norm=0.4324\n",
      "[iter 200] loss=-3.4856 val_loss=0.0000 scale=1.0000 norm=0.4245\n",
      "[iter 300] loss=-3.6732 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.7163 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.8091 val_loss=0.0000 scale=1.0000 norm=0.4224\n",
      "[iter 600] loss=-3.9325 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.9564 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.9789 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-4.0058 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-4.0186 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-4.0185 val_loss=0.0000 scale=1.0000 norm=0.4042\n",
      "[iter 1200] loss=-4.0372 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1300] loss=-4.0381 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-4.0415 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-4.0592 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.0745 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.0855 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1052 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1900] loss=-4.1146 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1151 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.1208 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-4.1288 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1353 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1364 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:49:46,189] Trial 34 finished with value: 8.762082447664356e-05 and parameters: {'n_estimators': 2402, 'learning_rate': 0.8409929729953441}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0899 val_loss=0.0000 scale=1.0000 norm=0.4337\n",
      "[iter 200] loss=-3.4317 val_loss=0.0000 scale=0.5000 norm=0.2135\n",
      "[iter 300] loss=-3.5663 val_loss=0.0000 scale=0.5000 norm=0.2094\n",
      "[iter 400] loss=-3.6599 val_loss=0.0000 scale=1.0000 norm=0.4257\n",
      "[iter 500] loss=-3.7786 val_loss=0.0000 scale=1.0000 norm=0.4132\n",
      "[iter 600] loss=-3.8358 val_loss=0.0000 scale=1.0000 norm=0.4079\n",
      "[iter 700] loss=-3.8601 val_loss=0.0000 scale=0.1250 norm=0.0513\n",
      "[iter 800] loss=-3.8966 val_loss=0.0000 scale=0.1250 norm=0.0509\n",
      "[iter 900] loss=-3.9212 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1000] loss=-3.9302 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-3.9427 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.9643 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-3.9671 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-3.9775 val_loss=0.0000 scale=2.0000 norm=0.8045\n",
      "[iter 1500] loss=-4.0046 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.0357 val_loss=0.0000 scale=0.0312 norm=0.0124\n",
      "[iter 1700] loss=-4.0476 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.0497 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.0668 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.0731 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.0845 val_loss=0.0000 scale=0.0312 norm=0.0122\n",
      "[iter 2200] loss=-4.0938 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.0932 val_loss=0.0000 scale=0.1250 norm=0.0492\n",
      "[iter 2400] loss=-4.1004 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.1097 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.1134 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1317 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.1465 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.1529 val_loss=0.0000 scale=0.1250 norm=0.0485\n",
      "[iter 3000] loss=-4.1634 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.1654 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.1690 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3300] loss=-4.1753 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.1772 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3500] loss=-4.1821 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3600] loss=-4.1902 val_loss=0.0000 scale=0.5000 norm=0.1935\n",
      "[iter 3700] loss=-4.1987 val_loss=0.0000 scale=0.5000 norm=0.1931\n",
      "[iter 3800] loss=-4.2000 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3900] loss=-4.2059 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4000] loss=-4.2064 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4100] loss=-4.2070 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 4200] loss=-4.2160 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:50:53,713] Trial 35 finished with value: 6.436652263171824e-05 and parameters: {'n_estimators': 4247, 'learning_rate': 0.7536939716763276}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2149 val_loss=0.0000 scale=1.0000 norm=0.4477\n",
      "[iter 200] loss=-3.5608 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 300] loss=-3.6696 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.7604 val_loss=0.0000 scale=1.0000 norm=0.4187\n",
      "[iter 500] loss=-3.8762 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 600] loss=-3.9119 val_loss=0.0000 scale=1.0000 norm=0.4085\n",
      "[iter 700] loss=-3.9283 val_loss=0.0000 scale=0.5000 norm=0.2003\n",
      "[iter 800] loss=-3.9334 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.9338 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.9470 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9793 val_loss=0.0000 scale=0.5000 norm=0.1969\n",
      "[iter 1200] loss=-4.0029 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0194 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.0336 val_loss=0.0000 scale=0.5000 norm=0.1979\n",
      "[iter 1500] loss=-4.0559 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.0670 val_loss=0.0000 scale=0.5000 norm=0.1973\n",
      "[iter 1700] loss=-4.0757 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.0759 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.0950 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1009 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.1077 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2200] loss=-4.1086 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1221 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1274 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-4.1275 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2600] loss=-4.1318 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2700] loss=-4.1394 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-4.1424 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:51:39,833] Trial 36 finished with value: 7.358420177269274e-05 and parameters: {'n_estimators': 2885, 'learning_rate': 0.9180469727910823}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0671 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 200] loss=-3.3049 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.3802 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.4315 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.4436 val_loss=0.0000 scale=0.2500 norm=0.1025\n",
      "[iter 600] loss=-3.5045 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.5481 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.5718 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.5692 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6129 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6496 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.6480 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.6875 val_loss=0.0000 scale=0.5000 norm=0.2016\n",
      "[iter 1400] loss=-3.7375 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-3.7781 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.7893 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.8357 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-3.8666 val_loss=0.0000 scale=0.5000 norm=0.2018\n",
      "[iter 1900] loss=-3.9025 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-3.9077 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.9140 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-3.9187 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-3.9158 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.9370 val_loss=0.0000 scale=0.5000 norm=0.1969\n",
      "[iter 2500] loss=-3.9523 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-3.9626 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2700] loss=-3.9741 val_loss=0.0000 scale=0.2500 norm=0.0988\n",
      "[iter 2800] loss=-3.9896 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.0033 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.0086 val_loss=0.0000 scale=0.0312 norm=0.0123\n",
      "[iter 3100] loss=-4.0197 val_loss=0.0000 scale=0.5000 norm=0.1966\n",
      "[iter 3200] loss=-4.0293 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.0321 val_loss=0.0000 scale=0.1250 norm=0.0489\n",
      "[iter 3400] loss=-4.0366 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3500] loss=-4.0514 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3600] loss=-4.0575 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 3700] loss=-4.0740 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3800] loss=-4.0877 val_loss=0.0000 scale=1.0000 norm=0.3884\n",
      "[iter 3900] loss=-4.0968 val_loss=0.0000 scale=0.2500 norm=0.0971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:52:47,089] Trial 37 finished with value: 6.45551845914794e-05 and parameters: {'n_estimators': 3988, 'learning_rate': 0.6519553081618527}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1116 val_loss=0.0000 scale=1.0000 norm=0.4379\n",
      "[iter 200] loss=-3.4867 val_loss=0.0000 scale=0.5000 norm=0.2078\n",
      "[iter 300] loss=-3.7170 val_loss=0.0000 scale=1.0000 norm=0.4161\n",
      "[iter 400] loss=-3.7823 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.7843 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 600] loss=-3.8008 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.8851 val_loss=0.0000 scale=1.0000 norm=0.4089\n",
      "[iter 800] loss=-3.9803 val_loss=0.0000 scale=2.0000 norm=0.8072\n",
      "[iter 900] loss=-4.0147 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-4.0315 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1100] loss=-4.0406 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1200] loss=-4.0666 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0759 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.0868 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 1500] loss=-4.1137 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.1329 val_loss=0.0000 scale=2.0000 norm=0.7889\n",
      "[iter 1700] loss=-4.1407 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1401 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.1626 val_loss=0.0000 scale=1.0000 norm=0.3911\n",
      "[iter 2000] loss=-4.1801 val_loss=0.0000 scale=0.5000 norm=0.1947\n",
      "[iter 2100] loss=-4.1953 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.2029 val_loss=0.0000 scale=2.0000 norm=0.7809\n",
      "[iter 2300] loss=-4.2240 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2400] loss=-4.2233 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-4.2475 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2600] loss=-4.2488 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.2468 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.2521 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.2649 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.2722 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.2719 val_loss=0.0000 scale=0.0039 norm=0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:53:37,038] Trial 38 finished with value: 6.123319750432096e-05 and parameters: {'n_estimators': 3134, 'learning_rate': 0.8447619972322953}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1983 val_loss=0.0000 scale=1.0000 norm=0.4146\n",
      "[iter 200] loss=-3.3785 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.4385 val_loss=0.0000 scale=0.5000 norm=0.2077\n",
      "[iter 400] loss=-3.5367 val_loss=0.0000 scale=0.2500 norm=0.1063\n",
      "[iter 500] loss=-3.5370 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 600] loss=-3.5646 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 700] loss=-3.5619 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 800] loss=-3.5748 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.6031 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 1000] loss=-3.6623 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 1100] loss=-3.6607 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 1200] loss=-3.6573 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 1300] loss=-3.6544 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 1400] loss=-3.6744 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 1500] loss=-3.6901 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 1600] loss=-3.6917 val_loss=0.0000 scale=1.0000 norm=0.4249\n",
      "[iter 1700] loss=-3.7265 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 1800] loss=-3.7240 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 1900] loss=-3.7223 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 2000] loss=-3.7447 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 2100] loss=-3.7799 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 2200] loss=-3.8101 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 2300] loss=-3.8101 val_loss=0.0000 scale=0.0039 norm=0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:54:18,189] Trial 39 finished with value: 0.00010880097937430734 and parameters: {'n_estimators': 2318, 'learning_rate': 0.5850967307525914}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1848 val_loss=0.0000 scale=1.0000 norm=0.4314\n",
      "[iter 200] loss=-3.5124 val_loss=0.0000 scale=1.0000 norm=0.4277\n",
      "[iter 300] loss=-3.7305 val_loss=0.0000 scale=1.0000 norm=0.4170\n",
      "[iter 400] loss=-3.8129 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.8273 val_loss=0.0000 scale=0.5000 norm=0.2075\n",
      "[iter 600] loss=-3.8778 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 700] loss=-3.9371 val_loss=0.0000 scale=1.0000 norm=0.4155\n",
      "[iter 800] loss=-3.9528 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 900] loss=-3.9824 val_loss=0.0000 scale=0.5000 norm=0.2058\n",
      "[iter 1000] loss=-3.9981 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-4.0023 val_loss=0.0000 scale=0.0156 norm=0.0064\n",
      "[iter 1200] loss=-4.0118 val_loss=0.0000 scale=0.5000 norm=0.2035\n",
      "[iter 1300] loss=-4.0314 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.0402 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.0536 val_loss=0.0000 scale=0.2500 norm=0.1008\n",
      "[iter 1600] loss=-4.0585 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.0803 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.0902 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.0983 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1075 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.1070 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2200] loss=-4.1227 val_loss=0.0000 scale=0.5000 norm=0.1972\n",
      "[iter 2300] loss=-4.1265 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1395 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.1437 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.1496 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1544 val_loss=0.0000 scale=0.0625 norm=0.0246\n",
      "[iter 2800] loss=-4.1571 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.1579 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3000] loss=-4.1653 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.1713 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3200] loss=-4.1725 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.1744 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.1803 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3500] loss=-4.1878 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3600] loss=-4.1980 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3700] loss=-4.2054 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3800] loss=-4.2154 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:55:20,032] Trial 40 finished with value: 6.661371409146411e-05 and parameters: {'n_estimators': 3826, 'learning_rate': 0.7420174925088028}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2101 val_loss=0.0000 scale=1.0000 norm=0.4385\n",
      "[iter 200] loss=-3.5594 val_loss=0.0000 scale=1.0000 norm=0.4159\n",
      "[iter 300] loss=-3.6412 val_loss=0.0000 scale=1.0000 norm=0.4185\n",
      "[iter 400] loss=-3.7851 val_loss=0.0000 scale=1.0000 norm=0.4140\n",
      "[iter 500] loss=-3.8282 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.9050 val_loss=0.0000 scale=1.0000 norm=0.4073\n",
      "[iter 700] loss=-3.9419 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.9577 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 900] loss=-3.9691 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.9733 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9947 val_loss=0.0000 scale=2.0000 norm=0.8064\n",
      "[iter 1200] loss=-4.0067 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0172 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.0447 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.0636 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.0817 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1700] loss=-4.1018 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1800] loss=-4.1193 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-4.1292 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-4.1373 val_loss=0.0000 scale=1.0000 norm=0.3992\n",
      "[iter 2100] loss=-4.1444 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 2200] loss=-4.1620 val_loss=0.0000 scale=0.2500 norm=0.1007\n",
      "[iter 2300] loss=-4.1824 val_loss=0.0000 scale=0.2500 norm=0.0996\n",
      "[iter 2400] loss=-4.1932 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-4.1961 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-4.1958 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1968 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.2113 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.2174 val_loss=0.0000 scale=0.5000 norm=0.1986\n",
      "[iter 3000] loss=-4.2222 val_loss=0.0000 scale=1.0000 norm=0.3951\n",
      "[iter 3100] loss=-4.2244 val_loss=0.0000 scale=0.5000 norm=0.1977\n",
      "[iter 3200] loss=-4.2368 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:56:12,413] Trial 41 finished with value: 5.775179513812409e-05 and parameters: {'n_estimators': 3272, 'learning_rate': 0.7372118842780503}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0264 val_loss=0.0000 scale=1.0000 norm=0.4326\n",
      "[iter 200] loss=-3.3249 val_loss=0.0000 scale=1.0000 norm=0.4324\n",
      "[iter 300] loss=-3.5686 val_loss=0.0000 scale=1.0000 norm=0.4214\n",
      "[iter 400] loss=-3.6923 val_loss=0.0000 scale=0.5000 norm=0.2068\n",
      "[iter 500] loss=-3.7697 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7923 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.8193 val_loss=0.0000 scale=0.2500 norm=0.1004\n",
      "[iter 800] loss=-3.8313 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.8325 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.8529 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.8871 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.9075 val_loss=0.0000 scale=0.2500 norm=0.1012\n",
      "[iter 1300] loss=-3.9396 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-3.9737 val_loss=0.0000 scale=0.2500 norm=0.1002\n",
      "[iter 1500] loss=-3.9895 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-4.0109 val_loss=0.0000 scale=0.5000 norm=0.1990\n",
      "[iter 1700] loss=-4.0410 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.0523 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.0642 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 2000] loss=-4.0780 val_loss=0.0000 scale=0.2500 norm=0.1002\n",
      "[iter 2100] loss=-4.0870 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.0972 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1075 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-4.1232 val_loss=0.0000 scale=0.2500 norm=0.0998\n",
      "[iter 2500] loss=-4.1310 val_loss=0.0000 scale=0.2500 norm=0.0996\n",
      "[iter 2600] loss=-4.1386 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1600 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.1711 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.1801 val_loss=0.0000 scale=0.5000 norm=0.1989\n",
      "[iter 3000] loss=-4.1868 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.1902 val_loss=0.0000 scale=0.5000 norm=0.1978\n",
      "[iter 3200] loss=-4.1998 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.2049 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 3400] loss=-4.2088 val_loss=0.0000 scale=0.0156 norm=0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:57:06,237] Trial 42 finished with value: 8.643436830619508e-05 and parameters: {'n_estimators': 3424, 'learning_rate': 0.8109687113061181}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2014 val_loss=0.0000 scale=1.0000 norm=0.4205\n",
      "[iter 200] loss=-3.4946 val_loss=0.0000 scale=1.0000 norm=0.4119\n",
      "[iter 300] loss=-3.5395 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 400] loss=-3.5713 val_loss=0.0000 scale=0.2500 norm=0.1041\n",
      "[iter 500] loss=-3.7269 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7432 val_loss=0.0000 scale=0.5000 norm=0.2025\n",
      "[iter 700] loss=-3.7669 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7793 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.8349 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.8697 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.8701 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8771 val_loss=0.0000 scale=1.0000 norm=0.4091\n",
      "[iter 1300] loss=-3.8903 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.9016 val_loss=0.0000 scale=1.0000 norm=0.4086\n",
      "[iter 1500] loss=-3.9111 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.9284 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-3.9399 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.9488 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9580 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-3.9701 val_loss=0.0000 scale=0.5000 norm=0.2013\n",
      "[iter 2100] loss=-3.9791 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9902 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2300] loss=-4.0079 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-4.0133 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-4.0377 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-4.0531 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2700] loss=-4.0618 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-4.0709 val_loss=0.0000 scale=0.0078 norm=0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:57:55,578] Trial 43 finished with value: 7.439046612189615e-05 and parameters: {'n_estimators': 2831, 'learning_rate': 0.7076342447996824}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0585 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 200] loss=-3.0666 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.1484 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 400] loss=-3.1510 val_loss=0.0000 scale=0.0020 norm=0.0009\n",
      "[iter 500] loss=-3.4067 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.4056 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 700] loss=-3.4058 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.4233 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.5522 val_loss=0.0000 scale=1.0000 norm=0.4134\n",
      "[iter 1000] loss=-3.5931 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6605 val_loss=0.0000 scale=0.2500 norm=0.1019\n",
      "[iter 1200] loss=-3.7139 val_loss=0.0000 scale=0.2500 norm=0.1031\n",
      "[iter 1300] loss=-3.7821 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-3.8637 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-3.8807 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8942 val_loss=0.0000 scale=1.0000 norm=0.4016\n",
      "[iter 1700] loss=-3.9014 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-3.9274 val_loss=0.0000 scale=0.1250 norm=0.0500\n",
      "[iter 1900] loss=-3.9476 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-3.9578 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2100] loss=-3.9584 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-3.9691 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-3.9764 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-3.9994 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2500] loss=-4.0156 val_loss=0.0000 scale=0.5000 norm=0.1959\n",
      "[iter 2600] loss=-4.0489 val_loss=0.0000 scale=0.2500 norm=0.0974\n",
      "[iter 2700] loss=-4.0841 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.0865 val_loss=0.0000 scale=1.0000 norm=0.3896\n",
      "[iter 2900] loss=-4.1022 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.1137 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.1157 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:58:47,589] Trial 44 finished with value: 6.816587861572826e-05 and parameters: {'n_estimators': 3117, 'learning_rate': 0.7681923865291552}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2577 val_loss=0.0000 scale=1.0000 norm=0.4203\n",
      "[iter 200] loss=-3.4700 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.5050 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 400] loss=-3.5582 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.6175 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6398 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.6934 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7089 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 900] loss=-3.7346 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.7564 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7579 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7661 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1300] loss=-3.7713 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.8043 val_loss=0.0000 scale=1.0000 norm=0.4012\n",
      "[iter 1500] loss=-3.8464 val_loss=0.0000 scale=0.5000 norm=0.2009\n",
      "[iter 1600] loss=-3.8447 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.8625 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8563 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.8750 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8818 val_loss=0.0000 scale=0.0312 norm=0.0123\n",
      "[iter 2100] loss=-3.9350 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-3.9355 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2300] loss=-3.9427 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2400] loss=-3.9397 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2500] loss=-3.9608 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2600] loss=-3.9723 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-3.9928 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-3.9911 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.0001 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3000] loss=-4.0032 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3100] loss=-4.0137 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.0176 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3300] loss=-4.0126 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3400] loss=-4.0260 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3500] loss=-4.0274 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3600] loss=-4.0299 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3700] loss=-4.0580 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3800] loss=-4.0689 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3900] loss=-4.0628 val_loss=0.0000 scale=1.0000 norm=0.3929\n",
      "[iter 4000] loss=-4.0750 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 4100] loss=-4.0789 val_loss=0.0000 scale=0.0039 norm=0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 11:59:59,428] Trial 45 finished with value: 7.297619786084232e-05 and parameters: {'n_estimators': 4180, 'learning_rate': 0.6754143213941383}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1033 val_loss=0.0000 scale=0.2500 norm=0.0995\n",
      "[iter 200] loss=-3.2402 val_loss=0.0000 scale=1.0000 norm=0.4019\n",
      "[iter 300] loss=-3.3212 val_loss=0.0000 scale=1.0000 norm=0.3939\n",
      "[iter 400] loss=-3.3402 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.5312 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.5307 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.5957 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.6088 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.6214 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6176 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6283 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.6429 val_loss=0.0000 scale=1.0000 norm=0.4030\n",
      "[iter 1300] loss=-3.6866 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7316 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7397 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.7804 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:00:30,131] Trial 46 finished with value: 0.0001091752668253701 and parameters: {'n_estimators': 1683, 'learning_rate': 0.5516455372229987}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0225 val_loss=0.0000 scale=1.0000 norm=0.4340\n",
      "[iter 200] loss=-3.2425 val_loss=0.0000 scale=1.0000 norm=0.4329\n",
      "[iter 300] loss=-3.5212 val_loss=0.0000 scale=1.0000 norm=0.4289\n",
      "[iter 400] loss=-3.6047 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.6487 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6760 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 700] loss=-3.6893 val_loss=0.0000 scale=0.5000 norm=0.2059\n",
      "[iter 800] loss=-3.6963 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7100 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7659 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.8071 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.8103 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-3.8242 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.8593 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-3.8626 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8669 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-3.8769 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1800] loss=-3.9239 val_loss=0.0000 scale=1.0000 norm=0.4040\n",
      "[iter 1900] loss=-3.9429 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 2000] loss=-3.9610 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.9809 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-3.9813 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-3.9899 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.0033 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.0101 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-4.0178 val_loss=0.0000 scale=0.5000 norm=0.1992\n",
      "[iter 2700] loss=-4.0272 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.0402 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-4.0506 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.0520 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.0739 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3200] loss=-4.0768 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3300] loss=-4.0852 val_loss=0.0000 scale=0.1250 norm=0.0493\n",
      "[iter 3400] loss=-4.0910 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3500] loss=-4.0963 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3600] loss=-4.0986 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3700] loss=-4.0981 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3800] loss=-4.1008 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3900] loss=-4.1054 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 4000] loss=-4.1069 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4100] loss=-4.1149 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4200] loss=-4.1205 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4300] loss=-4.1208 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4400] loss=-4.1255 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:01:44,638] Trial 47 finished with value: 8.699729772156066e-05 and parameters: {'n_estimators': 4462, 'learning_rate': 0.8276058608883795}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1436 val_loss=0.0000 scale=0.5000 norm=0.2098\n",
      "[iter 200] loss=-3.3890 val_loss=0.0000 scale=1.0000 norm=0.4182\n",
      "[iter 300] loss=-3.5911 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 400] loss=-3.5910 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.6048 val_loss=0.0000 scale=0.2500 norm=0.1033\n",
      "[iter 600] loss=-3.6253 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.6316 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.6395 val_loss=0.0000 scale=0.1250 norm=0.0512\n",
      "[iter 900] loss=-3.6399 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6466 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1100] loss=-3.6460 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.6969 val_loss=0.0000 scale=0.5000 norm=0.2060\n",
      "[iter 1300] loss=-3.7464 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7463 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7528 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-3.7841 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-3.7856 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8301 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-3.8443 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8495 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.8590 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.8583 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.8606 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.8712 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-3.8806 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-3.8937 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2700] loss=-3.8986 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-3.9146 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-3.9146 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-3.9354 val_loss=0.0000 scale=0.2500 norm=0.1007\n",
      "[iter 3100] loss=-3.9425 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3200] loss=-3.9423 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3300] loss=-3.9449 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 3400] loss=-3.9556 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3500] loss=-3.9562 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3600] loss=-3.9581 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:02:48,480] Trial 48 finished with value: 7.399906902864332e-05 and parameters: {'n_estimators': 3698, 'learning_rate': 0.49707471941041215}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.9086 val_loss=0.0000 scale=1.0000 norm=0.4470\n",
      "[iter 200] loss=-3.4244 val_loss=0.0000 scale=1.0000 norm=0.4306\n",
      "[iter 300] loss=-3.6305 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.6999 val_loss=0.0000 scale=0.0312 norm=0.0133\n",
      "[iter 500] loss=-3.7720 val_loss=0.0000 scale=1.0000 norm=0.4128\n",
      "[iter 600] loss=-3.8136 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 700] loss=-3.8378 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.8949 val_loss=0.0000 scale=1.0000 norm=0.4003\n",
      "[iter 900] loss=-3.9735 val_loss=0.0000 scale=0.2500 norm=0.1000\n",
      "[iter 1000] loss=-3.9891 val_loss=0.0000 scale=1.0000 norm=0.3991\n",
      "[iter 1100] loss=-4.0165 val_loss=0.0000 scale=1.0000 norm=0.3945\n",
      "[iter 1200] loss=-4.0476 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0682 val_loss=0.0000 scale=1.0000 norm=0.3907\n",
      "[iter 1400] loss=-4.0756 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 1500] loss=-4.0799 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 1600] loss=-4.0968 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1700] loss=-4.0958 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1800] loss=-4.1068 val_loss=0.0000 scale=0.5000 norm=0.1936\n",
      "[iter 1900] loss=-4.1106 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2000] loss=-4.1159 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2100] loss=-4.1243 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2200] loss=-4.1297 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2300] loss=-4.1336 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2400] loss=-4.1395 val_loss=0.0000 scale=0.2500 norm=0.0966\n",
      "[iter 2500] loss=-4.1432 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2600] loss=-4.1588 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.1616 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.1718 val_loss=0.0000 scale=1.0000 norm=0.3848\n",
      "[iter 2900] loss=-4.1750 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.1804 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.1870 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.1919 val_loss=0.0000 scale=1.0000 norm=0.3825\n",
      "[iter 3300] loss=-4.1966 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.2013 val_loss=0.0000 scale=1.0000 norm=0.3792\n",
      "[iter 3500] loss=-4.2067 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3600] loss=-4.2083 val_loss=0.0000 scale=1.0000 norm=0.3790\n",
      "[iter 3700] loss=-4.2219 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3800] loss=-4.2378 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3900] loss=-4.2417 val_loss=0.0000 scale=0.0625 norm=0.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:03:49,648] Trial 49 finished with value: 6.971489105420972e-05 and parameters: {'n_estimators': 3902, 'learning_rate': 0.9967410444032822}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1132 val_loss=0.0000 scale=0.5000 norm=0.2222\n",
      "[iter 200] loss=-3.4942 val_loss=0.0000 scale=1.0000 norm=0.4210\n",
      "[iter 300] loss=-3.6434 val_loss=0.0000 scale=0.2500 norm=0.1049\n",
      "[iter 400] loss=-3.7362 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.7958 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.8035 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.8271 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.8266 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 900] loss=-3.8665 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.8829 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:04:05,786] Trial 50 finished with value: 9.359104384710706e-05 and parameters: {'n_estimators': 1063, 'learning_rate': 0.7680066431787272}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1160 val_loss=0.0000 scale=1.0000 norm=0.4356\n",
      "[iter 200] loss=-3.5202 val_loss=0.0000 scale=1.0000 norm=0.4173\n",
      "[iter 300] loss=-3.6234 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.7365 val_loss=0.0000 scale=1.0000 norm=0.4207\n",
      "[iter 500] loss=-3.7728 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 600] loss=-3.7785 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.8076 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 800] loss=-3.8383 val_loss=0.0000 scale=0.5000 norm=0.2094\n",
      "[iter 900] loss=-3.8627 val_loss=0.0000 scale=0.5000 norm=0.2088\n",
      "[iter 1000] loss=-3.9147 val_loss=0.0000 scale=2.0000 norm=0.8237\n",
      "[iter 1100] loss=-3.9400 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.9619 val_loss=0.0000 scale=0.2500 norm=0.1021\n",
      "[iter 1300] loss=-3.9694 val_loss=0.0000 scale=0.5000 norm=0.2042\n",
      "[iter 1400] loss=-3.9819 val_loss=0.0000 scale=0.5000 norm=0.2042\n",
      "[iter 1500] loss=-3.9933 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-4.0101 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.0191 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.0574 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.0650 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.0852 val_loss=0.0000 scale=1.0000 norm=0.4006\n",
      "[iter 2100] loss=-4.0990 val_loss=0.0000 scale=0.5000 norm=0.1981\n",
      "[iter 2200] loss=-4.1181 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1361 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1508 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.1551 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.1589 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1654 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.1773 val_loss=0.0000 scale=0.5000 norm=0.1942\n",
      "[iter 2900] loss=-4.1829 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 3000] loss=-4.1864 val_loss=0.0000 scale=0.5000 norm=0.1934\n",
      "[iter 3100] loss=-4.2027 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.2146 val_loss=0.0000 scale=0.0312 norm=0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:04:56,861] Trial 51 finished with value: 6.48918111798285e-05 and parameters: {'n_estimators': 3295, 'learning_rate': 0.7275936796162624}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2010 val_loss=0.0000 scale=1.0000 norm=0.4339\n",
      "[iter 200] loss=-3.5404 val_loss=0.0000 scale=1.0000 norm=0.4241\n",
      "[iter 300] loss=-3.6062 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.6441 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.7342 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7327 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7853 val_loss=0.0000 scale=0.5000 norm=0.2076\n",
      "[iter 800] loss=-3.8204 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.8363 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.8520 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-3.8607 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.8875 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.8874 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.9148 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-3.9339 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.9354 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.9354 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.9378 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-3.9432 val_loss=0.0000 scale=0.1250 norm=0.0506\n",
      "[iter 2000] loss=-3.9689 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2100] loss=-3.9883 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9906 val_loss=0.0000 scale=0.5000 norm=0.2007\n",
      "[iter 2300] loss=-4.0289 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.0426 val_loss=0.0000 scale=0.5000 norm=0.1985\n",
      "[iter 2500] loss=-4.0450 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.0471 val_loss=0.0000 scale=0.0156 norm=0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:05:38,311] Trial 52 finished with value: 7.059004742206909e-05 and parameters: {'n_estimators': 2608, 'learning_rate': 0.6363612194246198}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.9595 val_loss=0.0000 scale=1.0000 norm=0.4305\n",
      "[iter 200] loss=-3.2980 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.3272 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.3754 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.4604 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.4832 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.4816 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.5264 val_loss=0.0000 scale=1.0000 norm=0.4121\n",
      "[iter 900] loss=-3.5496 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.5482 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.5809 val_loss=0.0000 scale=0.2500 norm=0.1041\n",
      "[iter 1200] loss=-3.6257 val_loss=0.0000 scale=2.0000 norm=0.8358\n",
      "[iter 1300] loss=-3.6993 val_loss=0.0000 scale=1.0000 norm=0.4110\n",
      "[iter 1400] loss=-3.7287 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-3.7291 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.7296 val_loss=0.0000 scale=0.0312 norm=0.0127\n",
      "[iter 1700] loss=-3.7894 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8126 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-3.8213 val_loss=0.0000 scale=1.0000 norm=0.4030\n",
      "[iter 2000] loss=-3.8472 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-3.8708 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-3.8840 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-3.8927 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.9072 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2500] loss=-3.9101 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-3.9128 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-3.9185 val_loss=0.0000 scale=0.2500 norm=0.0978\n",
      "[iter 2800] loss=-3.9355 val_loss=0.0000 scale=1.0000 norm=0.3927\n",
      "[iter 2900] loss=-3.9566 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:06:28,148] Trial 53 finished with value: 9.511025818874294e-05 and parameters: {'n_estimators': 3000, 'learning_rate': 0.7967017522154424}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2440 val_loss=0.0000 scale=1.0000 norm=0.4240\n",
      "[iter 200] loss=-3.3670 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.5106 val_loss=0.0000 scale=1.0000 norm=0.4146\n",
      "[iter 400] loss=-3.6186 val_loss=0.0000 scale=1.0000 norm=0.4101\n",
      "[iter 500] loss=-3.6549 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6639 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7204 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7338 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7541 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7617 val_loss=0.0000 scale=1.0000 norm=0.4066\n",
      "[iter 1100] loss=-3.7990 val_loss=0.0000 scale=1.0000 norm=0.4031\n",
      "[iter 1200] loss=-3.8368 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.8492 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1400] loss=-3.8531 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8956 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.9403 val_loss=0.0000 scale=0.2500 norm=0.1015\n",
      "[iter 1700] loss=-3.9668 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-3.9749 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9781 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-3.9887 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-4.0035 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.0126 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-4.0144 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-4.0153 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2500] loss=-4.0203 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.0362 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2700] loss=-4.0561 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-4.0704 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.0841 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3000] loss=-4.0952 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.0944 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.1201 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3300] loss=-4.1253 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:07:25,252] Trial 54 finished with value: 7.472199488389629e-05 and parameters: {'n_estimators': 3368, 'learning_rate': 0.6851866772075587}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1087 val_loss=0.0000 scale=0.5000 norm=0.2046\n",
      "[iter 200] loss=-3.2591 val_loss=0.0000 scale=0.2500 norm=0.1014\n",
      "[iter 300] loss=-3.2693 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 400] loss=-3.3309 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.4218 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.4695 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.4681 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.4801 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.4811 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6092 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6117 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.6291 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-3.6345 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.6522 val_loss=0.0000 scale=0.2500 norm=0.1021\n",
      "[iter 1500] loss=-3.6622 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1600] loss=-3.6606 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1700] loss=-3.6588 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1800] loss=-3.7003 val_loss=0.0000 scale=0.2500 norm=0.1029\n",
      "[iter 1900] loss=-3.7191 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.7304 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.7392 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2200] loss=-3.7403 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2300] loss=-3.7485 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.7570 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-3.7565 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-3.7625 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2700] loss=-3.7656 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-3.7703 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-3.7724 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-3.7688 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3100] loss=-3.7854 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3200] loss=-3.7878 val_loss=0.0000 scale=0.2500 norm=0.1022\n",
      "[iter 3300] loss=-3.8034 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3400] loss=-3.8113 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3500] loss=-3.8104 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3600] loss=-3.8109 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3700] loss=-3.8228 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3800] loss=-3.8493 val_loss=0.0000 scale=0.1250 norm=0.0509\n",
      "[iter 3900] loss=-3.8493 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 4000] loss=-3.8491 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4100] loss=-3.8726 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4200] loss=-3.8781 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4300] loss=-3.9013 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 4400] loss=-3.9136 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4500] loss=-3.9261 val_loss=0.0000 scale=1.0000 norm=0.4008\n",
      "[iter 4600] loss=-3.9413 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4700] loss=-3.9611 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:08:49,135] Trial 55 finished with value: 9.624264699877096e-05 and parameters: {'n_estimators': 4761, 'learning_rate': 0.6019908313838046}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0993 val_loss=0.0000 scale=1.0000 norm=0.4441\n",
      "[iter 200] loss=-3.4887 val_loss=0.0000 scale=0.5000 norm=0.2137\n",
      "[iter 300] loss=-3.6247 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.6227 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 500] loss=-3.6458 val_loss=0.0000 scale=0.5000 norm=0.2113\n",
      "[iter 600] loss=-3.7379 val_loss=0.0000 scale=0.0312 norm=0.0132\n",
      "[iter 700] loss=-3.7944 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.8712 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.8949 val_loss=0.0000 scale=1.0000 norm=0.4148\n",
      "[iter 1000] loss=-3.9610 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-4.0164 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-4.0368 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0423 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.0448 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.0793 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.0979 val_loss=0.0000 scale=2.0000 norm=0.7997\n",
      "[iter 1700] loss=-4.1067 val_loss=0.0000 scale=0.1250 norm=0.0499\n",
      "[iter 1800] loss=-4.1176 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:09:18,037] Trial 56 finished with value: 6.198197371796641e-05 and parameters: {'n_estimators': 1888, 'learning_rate': 0.7486506143857657}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.9833 val_loss=0.0000 scale=1.0000 norm=0.4412\n",
      "[iter 200] loss=-3.2076 val_loss=0.0000 scale=1.0000 norm=0.4237\n",
      "[iter 300] loss=-3.4499 val_loss=0.0000 scale=0.5000 norm=0.2030\n",
      "[iter 400] loss=-3.5303 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.5736 val_loss=0.0000 scale=1.0000 norm=0.4079\n",
      "[iter 600] loss=-3.6198 val_loss=0.0000 scale=1.0000 norm=0.4111\n",
      "[iter 700] loss=-3.6559 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7243 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7246 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.7736 val_loss=0.0000 scale=1.0000 norm=0.4050\n",
      "[iter 1100] loss=-3.8482 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8479 val_loss=0.0000 scale=0.1250 norm=0.0504\n",
      "[iter 1300] loss=-3.8780 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-3.9112 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.9207 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.9532 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.9591 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-3.9669 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-3.9649 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2000] loss=-3.9794 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-3.9804 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9937 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-4.0036 val_loss=0.0000 scale=2.0000 norm=0.7891\n",
      "[iter 2400] loss=-4.0351 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.0553 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2600] loss=-4.0609 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.0786 val_loss=0.0000 scale=0.2500 norm=0.0969\n",
      "[iter 2800] loss=-4.0834 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.0823 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3000] loss=-4.0905 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.1008 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.1041 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3300] loss=-4.1137 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.1151 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3500] loss=-4.1381 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3600] loss=-4.1408 val_loss=0.0000 scale=0.1250 norm=0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:10:16,689] Trial 57 finished with value: 8.308446454053328e-05 and parameters: {'n_estimators': 3611, 'learning_rate': 0.9529992477607171}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2016 val_loss=0.0000 scale=1.0000 norm=0.4509\n",
      "[iter 200] loss=-3.5788 val_loss=0.0000 scale=1.0000 norm=0.4268\n",
      "[iter 300] loss=-3.7687 val_loss=0.0000 scale=0.5000 norm=0.2094\n",
      "[iter 400] loss=-3.7968 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.8441 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 600] loss=-3.8764 val_loss=0.0000 scale=0.5000 norm=0.2075\n",
      "[iter 700] loss=-3.9175 val_loss=0.0000 scale=0.5000 norm=0.2055\n",
      "[iter 800] loss=-3.9625 val_loss=0.0000 scale=1.0000 norm=0.4078\n",
      "[iter 900] loss=-3.9788 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.9985 val_loss=0.0000 scale=1.0000 norm=0.4090\n",
      "[iter 1100] loss=-4.0072 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-4.0256 val_loss=0.0000 scale=0.1250 norm=0.0507\n",
      "[iter 1300] loss=-4.0418 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-4.0680 val_loss=0.0000 scale=0.5000 norm=0.2016\n",
      "[iter 1500] loss=-4.0798 val_loss=0.0000 scale=0.5000 norm=0.2004\n",
      "[iter 1600] loss=-4.1011 val_loss=0.0000 scale=0.5000 norm=0.1989\n",
      "[iter 1700] loss=-4.1075 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1238 val_loss=0.0000 scale=0.0312 norm=0.0125\n",
      "[iter 1900] loss=-4.1365 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1623 val_loss=0.0000 scale=1.0000 norm=0.3980\n",
      "[iter 2100] loss=-4.1660 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-4.1775 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2300] loss=-4.1910 val_loss=0.0000 scale=1.0000 norm=0.3963\n",
      "[iter 2400] loss=-4.2201 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.2296 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.2434 val_loss=0.0000 scale=0.2500 norm=0.0988\n",
      "[iter 2700] loss=-4.2526 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.2580 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2900] loss=-4.2694 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3000] loss=-4.2814 val_loss=0.0000 scale=0.5000 norm=0.1936\n",
      "[iter 3100] loss=-4.2829 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.2874 val_loss=0.0000 scale=0.0156 norm=0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:11:06,826] Trial 58 finished with value: 6.276236231903691e-05 and parameters: {'n_estimators': 3212, 'learning_rate': 0.8743620602497834}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1911 val_loss=0.0000 scale=1.0000 norm=0.4264\n",
      "[iter 200] loss=-3.3421 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 300] loss=-3.3921 val_loss=0.0000 scale=0.5000 norm=0.2097\n",
      "[iter 400] loss=-3.4682 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.5109 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 600] loss=-3.5282 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 700] loss=-3.5279 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 800] loss=-3.5492 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 900] loss=-3.5553 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.5662 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.5963 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1200] loss=-3.6697 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7216 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7205 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7459 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.7685 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-3.7831 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8043 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.8351 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8639 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.8779 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2200] loss=-3.8775 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.8886 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-3.8928 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-3.9099 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-3.9212 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-3.9246 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2800] loss=-3.9339 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-3.9457 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-3.9677 val_loss=0.0000 scale=1.0000 norm=0.4030\n",
      "[iter 3100] loss=-3.9744 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3200] loss=-4.0000 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3300] loss=-4.0182 val_loss=0.0000 scale=0.1250 norm=0.0500\n",
      "[iter 3400] loss=-4.0274 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3500] loss=-4.0304 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3600] loss=-4.0407 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3700] loss=-4.0499 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3800] loss=-4.0509 val_loss=0.0000 scale=1.0000 norm=0.3992\n",
      "[iter 3900] loss=-4.0521 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 4000] loss=-4.0556 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4100] loss=-4.0596 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4200] loss=-4.0677 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4300] loss=-4.0686 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:12:19,194] Trial 59 finished with value: 8.053449074189344e-05 and parameters: {'n_estimators': 4327, 'learning_rate': 0.6991608369821641}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.9736 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 200] loss=-3.0412 val_loss=0.0000 scale=0.5000 norm=0.2128\n",
      "[iter 300] loss=-3.1309 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.2767 val_loss=0.0000 scale=0.5000 norm=0.2060\n",
      "[iter 500] loss=-3.3429 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.4379 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.4944 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.5246 val_loss=0.0000 scale=0.5000 norm=0.2030\n",
      "[iter 900] loss=-3.6508 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6905 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6927 val_loss=0.0000 scale=1.0000 norm=0.3999\n",
      "[iter 1200] loss=-3.7243 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7391 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-3.7467 val_loss=0.0000 scale=1.0000 norm=0.3956\n",
      "[iter 1500] loss=-3.7807 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1600] loss=-3.7960 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.8102 val_loss=0.0000 scale=1.0000 norm=0.3973\n",
      "[iter 1800] loss=-3.8229 val_loss=0.0000 scale=1.0000 norm=0.3988\n",
      "[iter 1900] loss=-3.8385 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8430 val_loss=0.0000 scale=0.5000 norm=0.1991\n",
      "[iter 2100] loss=-3.8813 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-3.9014 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2300] loss=-3.9123 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-3.9231 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-3.9353 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2600] loss=-3.9685 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2700] loss=-3.9901 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.0026 val_loss=0.0000 scale=0.1250 norm=0.0486\n",
      "[iter 2900] loss=-4.0180 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 3000] loss=-4.0249 val_loss=0.0000 scale=1.0000 norm=0.3862\n",
      "[iter 3100] loss=-4.0322 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.0362 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3300] loss=-4.0445 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.0590 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3500] loss=-4.0726 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3600] loss=-4.0761 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3700] loss=-4.0797 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3800] loss=-4.0781 val_loss=0.0000 scale=0.5000 norm=0.1927\n",
      "[iter 3900] loss=-4.0937 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:13:25,925] Trial 60 finished with value: 7.299713574354936e-05 and parameters: {'n_estimators': 3995, 'learning_rate': 0.6603420191081132}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2157 val_loss=0.0000 scale=1.0000 norm=0.4386\n",
      "[iter 200] loss=-3.5490 val_loss=0.0000 scale=0.5000 norm=0.2053\n",
      "[iter 300] loss=-3.6682 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.7647 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 500] loss=-3.8176 val_loss=0.0000 scale=1.0000 norm=0.4082\n",
      "[iter 600] loss=-3.8198 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.8486 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.8700 val_loss=0.0000 scale=0.1250 norm=0.0507\n",
      "[iter 900] loss=-3.9205 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.9194 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-3.9359 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.9673 val_loss=0.0000 scale=1.0000 norm=0.3994\n",
      "[iter 1300] loss=-3.9770 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.9874 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1500] loss=-3.9894 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1600] loss=-4.0054 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-4.0105 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1800] loss=-4.0109 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.0093 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.0067 val_loss=0.0000 scale=0.5000 norm=0.1991\n",
      "[iter 2100] loss=-4.0130 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-4.0316 val_loss=0.0000 scale=0.2500 norm=0.0989\n",
      "[iter 2300] loss=-4.0380 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2400] loss=-4.0432 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.0435 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.0456 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2700] loss=-4.0620 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.0722 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2900] loss=-4.0711 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3000] loss=-4.0765 val_loss=0.0000 scale=1.0000 norm=0.3947\n",
      "[iter 3100] loss=-4.0864 val_loss=0.0000 scale=0.0039 norm=0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:14:17,936] Trial 61 finished with value: 7.60029373441298e-05 and parameters: {'n_estimators': 3180, 'learning_rate': 0.8475258847470113}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0451 val_loss=0.0000 scale=1.0000 norm=0.4385\n",
      "[iter 200] loss=-3.0549 val_loss=0.0000 scale=0.0020 norm=0.0009\n",
      "[iter 300] loss=-3.3113 val_loss=0.0000 scale=1.0000 norm=0.4267\n",
      "[iter 400] loss=-3.3626 val_loss=0.0000 scale=1.0000 norm=0.4248\n",
      "[iter 500] loss=-3.3833 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 600] loss=-3.4322 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 700] loss=-3.5722 val_loss=0.0000 scale=1.0000 norm=0.4195\n",
      "[iter 800] loss=-3.6691 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7170 val_loss=0.0000 scale=1.0000 norm=0.4107\n",
      "[iter 1000] loss=-3.7983 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-3.8429 val_loss=0.0000 scale=1.0000 norm=0.4089\n",
      "[iter 1200] loss=-3.8986 val_loss=0.0000 scale=0.2500 norm=0.1009\n",
      "[iter 1300] loss=-3.9467 val_loss=0.0000 scale=0.5000 norm=0.2014\n",
      "[iter 1400] loss=-3.9668 val_loss=0.0000 scale=0.5000 norm=0.2010\n",
      "[iter 1500] loss=-3.9743 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-4.0142 val_loss=0.0000 scale=1.0000 norm=0.4028\n",
      "[iter 1700] loss=-4.0185 val_loss=0.0000 scale=1.0000 norm=0.4021\n",
      "[iter 1800] loss=-4.0319 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.0587 val_loss=0.0000 scale=0.0312 norm=0.0124\n",
      "[iter 2000] loss=-4.0607 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.0865 val_loss=0.0000 scale=0.1250 norm=0.0499\n",
      "[iter 2200] loss=-4.0903 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-4.0895 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1043 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.1113 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.1209 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1244 val_loss=0.0000 scale=1.0000 norm=0.3987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:15:02,632] Trial 62 finished with value: 8.373481525623127e-05 and parameters: {'n_estimators': 2789, 'learning_rate': 0.8009647275758821}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2114 val_loss=0.0000 scale=0.5000 norm=0.2187\n",
      "[iter 200] loss=-3.5476 val_loss=0.0000 scale=1.0000 norm=0.4114\n",
      "[iter 300] loss=-3.5775 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.6118 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.6388 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6924 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7303 val_loss=0.0000 scale=0.5000 norm=0.2058\n",
      "[iter 800] loss=-3.7593 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7615 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.8166 val_loss=0.0000 scale=1.0000 norm=0.4059\n",
      "[iter 1100] loss=-3.8480 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8658 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.9075 val_loss=0.0000 scale=0.2500 norm=0.1008\n",
      "[iter 1400] loss=-3.9325 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-3.9646 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-3.9666 val_loss=0.0000 scale=1.0000 norm=0.3982\n",
      "[iter 1700] loss=-3.9764 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.9858 val_loss=0.0000 scale=0.2500 norm=0.0994\n",
      "[iter 1900] loss=-3.9902 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.9955 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.0027 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-4.0153 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-4.0249 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-4.0519 val_loss=0.0000 scale=1.0000 norm=0.3948\n",
      "[iter 2500] loss=-4.0759 val_loss=0.0000 scale=0.2500 norm=0.0995\n",
      "[iter 2600] loss=-4.0982 val_loss=0.0000 scale=0.5000 norm=0.1994\n",
      "[iter 2700] loss=-4.0990 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.1140 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.1180 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.1197 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:15:52,023] Trial 63 finished with value: 7.111920566239743e-05 and parameters: {'n_estimators': 3001, 'learning_rate': 0.8475878638920675}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2423 val_loss=0.0000 scale=1.0000 norm=0.4367\n",
      "[iter 200] loss=-3.5431 val_loss=0.0000 scale=1.0000 norm=0.4261\n",
      "[iter 300] loss=-3.6568 val_loss=0.0000 scale=1.0000 norm=0.4210\n",
      "[iter 400] loss=-3.7849 val_loss=0.0000 scale=1.0000 norm=0.4260\n",
      "[iter 500] loss=-3.8629 val_loss=0.0000 scale=0.5000 norm=0.2128\n",
      "[iter 600] loss=-3.9471 val_loss=0.0000 scale=1.0000 norm=0.4182\n",
      "[iter 700] loss=-3.9726 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.9835 val_loss=0.0000 scale=1.0000 norm=0.4148\n",
      "[iter 900] loss=-3.9960 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.9988 val_loss=0.0000 scale=0.0156 norm=0.0065\n",
      "[iter 1100] loss=-4.0107 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-4.0156 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-4.0428 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-4.0699 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-4.0786 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-4.0824 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-4.0976 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1800] loss=-4.1127 val_loss=0.0000 scale=0.0625 norm=0.0253\n",
      "[iter 1900] loss=-4.1232 val_loss=0.0000 scale=0.2500 norm=0.1014\n",
      "[iter 2000] loss=-4.1574 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-4.1786 val_loss=0.0000 scale=0.5000 norm=0.2008\n",
      "[iter 2200] loss=-4.1861 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1934 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.2000 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2500] loss=-4.2041 val_loss=0.0000 scale=0.5000 norm=0.1996\n",
      "[iter 2600] loss=-4.2107 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2700] loss=-4.2121 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.2188 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.2238 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.2345 val_loss=0.0000 scale=0.2500 norm=0.0997\n",
      "[iter 3100] loss=-4.2379 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3200] loss=-4.2401 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 3300] loss=-4.2451 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.2458 val_loss=0.0000 scale=0.5000 norm=0.1977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:16:46,224] Trial 64 finished with value: 5.675792947094915e-05 and parameters: {'n_estimators': 3427, 'learning_rate': 0.7333207730067781}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1278 val_loss=0.0000 scale=1.0000 norm=0.4371\n",
      "[iter 200] loss=-3.4782 val_loss=0.0000 scale=1.0000 norm=0.4310\n",
      "[iter 300] loss=-3.6826 val_loss=0.0000 scale=0.5000 norm=0.2089\n",
      "[iter 400] loss=-3.8045 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.8199 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.9038 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 700] loss=-3.9204 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.9339 val_loss=0.0000 scale=1.0000 norm=0.4102\n",
      "[iter 900] loss=-3.9592 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-4.0001 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-4.0271 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-4.0349 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-4.0575 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-4.0871 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-4.0868 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1600] loss=-4.0916 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-4.1087 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1166 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1900] loss=-4.1335 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1329 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.1349 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.1471 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1486 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1594 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-4.1592 val_loss=0.0000 scale=1.0000 norm=0.4003\n",
      "[iter 2600] loss=-4.1640 val_loss=0.0000 scale=0.5000 norm=0.1991\n",
      "[iter 2700] loss=-4.1764 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.1831 val_loss=0.0000 scale=1.0000 norm=0.3948\n",
      "[iter 2900] loss=-4.1856 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 3000] loss=-4.1867 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.1878 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3200] loss=-4.1887 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.1909 val_loss=0.0000 scale=1.0000 norm=0.3912\n",
      "[iter 3400] loss=-4.2014 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3500] loss=-4.2055 val_loss=0.0000 scale=0.0039 norm=0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:17:41,425] Trial 65 finished with value: 6.989467126013692e-05 and parameters: {'n_estimators': 3514, 'learning_rate': 0.7298016608815463}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1138 val_loss=0.0000 scale=1.0000 norm=0.4303\n",
      "[iter 200] loss=-3.3281 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.4429 val_loss=0.0000 scale=2.0000 norm=0.8413\n",
      "[iter 400] loss=-3.5746 val_loss=0.0000 scale=0.5000 norm=0.2031\n",
      "[iter 500] loss=-3.5974 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.5967 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.6347 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.6685 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.6980 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.7856 val_loss=0.0000 scale=0.5000 norm=0.2010\n",
      "[iter 1100] loss=-3.8631 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8845 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.8896 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-3.9041 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-3.9234 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-3.9390 val_loss=0.0000 scale=1.0000 norm=0.3935\n",
      "[iter 1700] loss=-3.9537 val_loss=0.0000 scale=0.0625 norm=0.0249\n",
      "[iter 1800] loss=-3.9733 val_loss=0.0000 scale=0.5000 norm=0.1970\n",
      "[iter 1900] loss=-3.9844 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2000] loss=-3.9956 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-3.9993 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-3.9981 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.0052 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2400] loss=-4.0162 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2500] loss=-4.0140 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2600] loss=-4.0208 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.0390 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-4.0466 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2900] loss=-4.0467 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.0460 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3100] loss=-4.0453 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.0557 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 3300] loss=-4.0572 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.0569 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:18:38,045] Trial 66 finished with value: 8.278130385168099e-05 and parameters: {'n_estimators': 3457, 'learning_rate': 0.7708453054357016}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2158 val_loss=0.0000 scale=1.0000 norm=0.4281\n",
      "[iter 200] loss=-3.4211 val_loss=0.0000 scale=1.0000 norm=0.4325\n",
      "[iter 300] loss=-3.6004 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.6362 val_loss=0.0000 scale=0.5000 norm=0.2141\n",
      "[iter 500] loss=-3.6940 val_loss=0.0000 scale=0.5000 norm=0.2100\n",
      "[iter 600] loss=-3.7384 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7674 val_loss=0.0000 scale=0.5000 norm=0.2099\n",
      "[iter 800] loss=-3.8188 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 900] loss=-3.8534 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.8526 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-3.8850 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.9129 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-3.9200 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-3.9204 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.9451 val_loss=0.0000 scale=1.0000 norm=0.4107\n",
      "[iter 1600] loss=-3.9529 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-3.9587 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1800] loss=-3.9590 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9657 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.9907 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2100] loss=-4.0006 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-4.0187 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-4.0322 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-4.0313 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2500] loss=-4.0432 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.0481 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.0543 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-4.0519 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-4.0513 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-4.0631 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3100] loss=-4.0782 val_loss=0.0000 scale=2.0000 norm=0.7976\n",
      "[iter 3200] loss=-4.0826 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3300] loss=-4.0918 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.0976 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3500] loss=-4.1058 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3600] loss=-4.1056 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:19:40,556] Trial 67 finished with value: 7.083511679536615e-05 and parameters: {'n_estimators': 3643, 'learning_rate': 0.7133577076520089}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1760 val_loss=0.0000 scale=0.5000 norm=0.2134\n",
      "[iter 200] loss=-3.2620 val_loss=0.0000 scale=0.0625 norm=0.0264\n",
      "[iter 300] loss=-3.3220 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 400] loss=-3.3273 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.3712 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.3844 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.5110 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.5315 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.5737 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1000] loss=-3.5815 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6063 val_loss=0.0000 scale=0.1250 norm=0.0503\n",
      "[iter 1200] loss=-3.6116 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.6255 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.6476 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-3.6490 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.6574 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.6604 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.6976 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.6938 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.7596 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.7554 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.7893 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.7932 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-3.8066 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-3.8283 val_loss=0.0000 scale=1.0000 norm=0.3958\n",
      "[iter 2600] loss=-3.8394 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:20:26,097] Trial 68 finished with value: 9.72182456943368e-05 and parameters: {'n_estimators': 2637, 'learning_rate': 0.629374327057324}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1543 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 200] loss=-3.2371 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.3527 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.3879 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.4103 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.4817 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 700] loss=-3.5115 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 800] loss=-3.5299 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.5388 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.5455 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.5500 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.5675 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.5828 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.5879 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.6227 val_loss=0.0000 scale=1.0000 norm=0.4098\n",
      "[iter 1600] loss=-3.6869 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.7183 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.7392 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.7375 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.7810 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.7997 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.7987 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.8148 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.8147 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-3.8159 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:21:10,859] Trial 69 finished with value: 0.00011434560808194128 and parameters: {'n_estimators': 2507, 'learning_rate': 0.6866700056192172}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0264 val_loss=0.0000 scale=1.0000 norm=0.4326\n",
      "[iter 200] loss=-3.3250 val_loss=0.0000 scale=1.0000 norm=0.4324\n",
      "[iter 300] loss=-3.5687 val_loss=0.0000 scale=1.0000 norm=0.4214\n",
      "[iter 400] loss=-3.6923 val_loss=0.0000 scale=0.5000 norm=0.2068\n",
      "[iter 500] loss=-3.7939 val_loss=0.0000 scale=0.1250 norm=0.0503\n",
      "[iter 600] loss=-3.8395 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 700] loss=-3.8617 val_loss=0.0000 scale=1.0000 norm=0.4018\n",
      "[iter 800] loss=-3.8927 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.9074 val_loss=0.0000 scale=0.5000 norm=0.2022\n",
      "[iter 1000] loss=-3.9221 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9473 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1200] loss=-3.9605 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.9623 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.9616 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-3.9836 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-3.9930 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.0110 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-4.0371 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.0419 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.0553 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.0663 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.0658 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1007 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-4.1113 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-4.1148 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.1257 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1488 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.1525 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.1589 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.1696 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.1761 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3200] loss=-4.1778 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.1856 val_loss=0.0000 scale=0.5000 norm=0.1973\n",
      "[iter 3400] loss=-4.2046 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3500] loss=-4.2077 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3600] loss=-4.2102 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3700] loss=-4.2123 val_loss=0.0000 scale=1.0000 norm=0.3928\n",
      "[iter 3800] loss=-4.2203 val_loss=0.0000 scale=1.0000 norm=0.3917\n",
      "[iter 3900] loss=-4.2310 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4000] loss=-4.2342 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 4100] loss=-4.2344 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 4200] loss=-4.2410 val_loss=0.0000 scale=0.2500 norm=0.0975\n",
      "[iter 4300] loss=-4.2476 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 4400] loss=-4.2474 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 4500] loss=-4.2711 val_loss=0.0000 scale=1.0000 norm=0.3934\n",
      "[iter 4600] loss=-4.2732 val_loss=0.0000 scale=0.1250 norm=0.0493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:22:24,507] Trial 70 finished with value: 8.497176042300719e-05 and parameters: {'n_estimators': 4636, 'learning_rate': 0.8109398155729081}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2050 val_loss=0.0000 scale=1.0000 norm=0.4467\n",
      "[iter 200] loss=-3.5824 val_loss=0.0000 scale=1.0000 norm=0.4260\n",
      "[iter 300] loss=-3.7105 val_loss=0.0000 scale=0.2500 norm=0.1042\n",
      "[iter 400] loss=-3.8045 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 500] loss=-3.8567 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.9000 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.9061 val_loss=0.0000 scale=1.0000 norm=0.3989\n",
      "[iter 800] loss=-3.9321 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 900] loss=-3.9493 val_loss=0.0000 scale=0.5000 norm=0.1976\n",
      "[iter 1000] loss=-3.9801 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9905 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-3.9944 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1300] loss=-3.9948 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.0361 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1500] loss=-4.0421 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.0547 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.0690 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1800] loss=-4.0838 val_loss=0.0000 scale=0.5000 norm=0.1950\n",
      "[iter 1900] loss=-4.1098 val_loss=0.0000 scale=0.5000 norm=0.1952\n",
      "[iter 2000] loss=-4.1258 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2100] loss=-4.1473 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2200] loss=-4.1642 val_loss=0.0000 scale=1.0000 norm=0.3863\n",
      "[iter 2300] loss=-4.1837 val_loss=0.0000 scale=0.5000 norm=0.1953\n",
      "[iter 2400] loss=-4.1905 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.2011 val_loss=0.0000 scale=0.5000 norm=0.1948\n",
      "[iter 2600] loss=-4.2160 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2700] loss=-4.2201 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2800] loss=-4.2234 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.2298 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.2398 val_loss=0.0000 scale=0.1250 norm=0.0487\n",
      "[iter 3100] loss=-4.2518 val_loss=0.0000 scale=0.0156 norm=0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:23:12,342] Trial 71 finished with value: 6.529116324467517e-05 and parameters: {'n_estimators': 3110, 'learning_rate': 0.7776338806723506}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2192 val_loss=0.0000 scale=1.0000 norm=0.4460\n",
      "[iter 200] loss=-3.5809 val_loss=0.0000 scale=0.1250 norm=0.0528\n",
      "[iter 300] loss=-3.7449 val_loss=0.0000 scale=1.0000 norm=0.4188\n",
      "[iter 400] loss=-3.8247 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.8777 val_loss=0.0000 scale=0.2500 norm=0.1042\n",
      "[iter 600] loss=-3.9483 val_loss=0.0000 scale=0.5000 norm=0.2057\n",
      "[iter 700] loss=-3.9803 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.9930 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-4.0245 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1000] loss=-4.0500 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1100] loss=-4.0760 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-4.0816 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1300] loss=-4.0871 val_loss=0.0000 scale=1.0000 norm=0.3932\n",
      "[iter 1400] loss=-4.0992 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.1086 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.1194 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1700] loss=-4.1375 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 1800] loss=-4.1427 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 1900] loss=-4.1551 val_loss=0.0000 scale=0.1250 norm=0.0489\n",
      "[iter 2000] loss=-4.1563 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2100] loss=-4.1609 val_loss=0.0000 scale=0.5000 norm=0.1948\n",
      "[iter 2200] loss=-4.1850 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2300] loss=-4.1939 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2400] loss=-4.2064 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 2500] loss=-4.2085 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 2600] loss=-4.2172 val_loss=0.0000 scale=1.0000 norm=0.3827\n",
      "[iter 2700] loss=-4.2233 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.2248 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 2900] loss=-4.2286 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.2329 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3100] loss=-4.2399 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.2452 val_loss=0.0000 scale=1.0000 norm=0.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:24:03,227] Trial 72 finished with value: 6.975474438970206e-05 and parameters: {'n_estimators': 3278, 'learning_rate': 0.8967785235228918}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1628 val_loss=0.0000 scale=1.0000 norm=0.4350\n",
      "[iter 200] loss=-3.5378 val_loss=0.0000 scale=0.5000 norm=0.2069\n",
      "[iter 300] loss=-3.6543 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.6712 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.6700 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6843 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7225 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7285 val_loss=0.0000 scale=1.0000 norm=0.4089\n",
      "[iter 900] loss=-3.7961 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.8501 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-3.8582 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8744 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-3.8832 val_loss=0.0000 scale=1.0000 norm=0.4072\n",
      "[iter 1400] loss=-3.9057 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.9105 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-3.9633 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1700] loss=-3.9813 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.9811 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9895 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.9938 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.9975 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9995 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-4.0120 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-4.0253 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.0432 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.0578 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.0595 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 2800] loss=-4.0783 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-4.0958 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3000] loss=-4.1041 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3100] loss=-4.1180 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3200] loss=-4.1285 val_loss=0.0000 scale=0.0312 norm=0.0125\n",
      "[iter 3300] loss=-4.1332 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.1500 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3500] loss=-4.1624 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3600] loss=-4.1728 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3700] loss=-4.1728 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3800] loss=-4.1799 val_loss=0.0000 scale=0.5000 norm=0.2015\n",
      "[iter 3900] loss=-4.1850 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 4000] loss=-4.1880 val_loss=0.0000 scale=0.0078 norm=0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:25:09,861] Trial 73 finished with value: 6.89459402964051e-05 and parameters: {'n_estimators': 4088, 'learning_rate': 0.7501940990907442}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0147 val_loss=0.0000 scale=1.0000 norm=0.4408\n",
      "[iter 200] loss=-3.3904 val_loss=0.0000 scale=1.0000 norm=0.4233\n",
      "[iter 300] loss=-3.5819 val_loss=0.0000 scale=0.2500 norm=0.1037\n",
      "[iter 400] loss=-3.5944 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.6298 val_loss=0.0000 scale=1.0000 norm=0.4113\n",
      "[iter 600] loss=-3.7225 val_loss=0.0000 scale=1.0000 norm=0.4103\n",
      "[iter 700] loss=-3.7716 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.8161 val_loss=0.0000 scale=1.0000 norm=0.4044\n",
      "[iter 900] loss=-3.8427 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.8468 val_loss=0.0000 scale=0.1250 norm=0.0507\n",
      "[iter 1100] loss=-3.8808 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-3.8826 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-3.8929 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-3.9253 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-3.9388 val_loss=0.0000 scale=1.0000 norm=0.3986\n",
      "[iter 1600] loss=-3.9424 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.9558 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.9596 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9649 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-3.9647 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.9717 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9759 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-3.9851 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.9976 val_loss=0.0000 scale=0.2500 norm=0.1008\n",
      "[iter 2500] loss=-4.0104 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-4.0189 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2700] loss=-4.0263 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.0339 val_loss=0.0000 scale=1.0000 norm=0.3931\n",
      "[iter 2900] loss=-4.0506 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.0578 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:26:00,046] Trial 74 finished with value: 9.917166310742448e-05 and parameters: {'n_estimators': 3080, 'learning_rate': 0.8262289062485284}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2109 val_loss=0.0000 scale=1.0000 norm=0.4352\n",
      "[iter 200] loss=-3.4251 val_loss=0.0000 scale=0.0312 norm=0.0130\n",
      "[iter 300] loss=-3.5100 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.5151 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.5759 val_loss=0.0000 scale=1.0000 norm=0.4109\n",
      "[iter 600] loss=-3.6583 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.6883 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7118 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7099 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7295 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1100] loss=-3.7283 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 1200] loss=-3.7240 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7539 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7621 val_loss=0.0000 scale=0.5000 norm=0.2040\n",
      "[iter 1500] loss=-3.7966 val_loss=0.0000 scale=0.0312 norm=0.0127\n",
      "[iter 1600] loss=-3.8219 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-3.8387 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1800] loss=-3.8534 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.8519 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.8719 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.8988 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9174 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.9280 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.9482 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2500] loss=-3.9674 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-3.9727 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2700] loss=-3.9911 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-3.9939 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-3.9972 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3000] loss=-3.9993 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3100] loss=-4.0094 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3200] loss=-4.0149 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3300] loss=-4.0238 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3400] loss=-4.0277 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3500] loss=-4.0403 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3600] loss=-4.0435 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3700] loss=-4.0616 val_loss=0.0000 scale=0.0312 norm=0.0126\n",
      "[iter 3800] loss=-4.0625 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:27:05,210] Trial 75 finished with value: 6.74319480023187e-05 and parameters: {'n_estimators': 3838, 'learning_rate': 0.6631878537383324}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0865 val_loss=0.0000 scale=1.0000 norm=0.4288\n",
      "[iter 200] loss=-3.2925 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 300] loss=-3.3938 val_loss=0.0000 scale=0.5000 norm=0.2135\n",
      "[iter 400] loss=-3.5583 val_loss=0.0000 scale=1.0000 norm=0.4230\n",
      "[iter 500] loss=-3.5853 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 600] loss=-3.6571 val_loss=0.0000 scale=1.0000 norm=0.4151\n",
      "[iter 700] loss=-3.6813 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.6935 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7591 val_loss=0.0000 scale=0.2500 norm=0.1019\n",
      "[iter 1000] loss=-3.7757 val_loss=0.0000 scale=2.0000 norm=0.8169\n",
      "[iter 1100] loss=-3.7964 val_loss=0.0000 scale=0.2500 norm=0.1025\n",
      "[iter 1200] loss=-3.8103 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:27:26,819] Trial 76 finished with value: 0.00011323027811524685 and parameters: {'n_estimators': 1254, 'learning_rate': 0.795924987367283}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-2.6550 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 200] loss=-3.2099 val_loss=0.0000 scale=1.0000 norm=0.4358\n",
      "[iter 300] loss=-3.5414 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.6872 val_loss=0.0000 scale=0.5000 norm=0.2130\n",
      "[iter 500] loss=-3.7572 val_loss=0.0000 scale=2.0000 norm=0.8412\n",
      "[iter 600] loss=-3.8103 val_loss=0.0000 scale=0.5000 norm=0.2099\n",
      "[iter 700] loss=-3.8339 val_loss=0.0000 scale=0.0625 norm=0.0261\n",
      "[iter 800] loss=-3.8763 val_loss=0.0000 scale=1.0000 norm=0.4167\n",
      "[iter 900] loss=-3.9087 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.9280 val_loss=0.0000 scale=0.1250 norm=0.0520\n",
      "[iter 1100] loss=-3.9467 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.9666 val_loss=0.0000 scale=1.0000 norm=0.4125\n",
      "[iter 1300] loss=-3.9867 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-3.9985 val_loss=0.0000 scale=0.0156 norm=0.0064\n",
      "[iter 1500] loss=-4.0272 val_loss=0.0000 scale=0.1250 norm=0.0511\n",
      "[iter 1600] loss=-4.0480 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-4.0726 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-4.0982 val_loss=0.0000 scale=0.5000 norm=0.2011\n",
      "[iter 1900] loss=-4.1105 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 2000] loss=-4.1338 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.1393 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 2200] loss=-4.1439 val_loss=0.0000 scale=0.2500 norm=0.0997\n",
      "[iter 2300] loss=-4.1591 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1649 val_loss=0.0000 scale=1.0000 norm=0.4035\n",
      "[iter 2500] loss=-4.1664 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-4.1899 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.2104 val_loss=0.0000 scale=1.0000 norm=0.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:28:10,207] Trial 77 finished with value: 6.530719769501486e-05 and parameters: {'n_estimators': 2734, 'learning_rate': 0.8559082579686295}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1473 val_loss=0.0000 scale=1.0000 norm=0.4411\n",
      "[iter 200] loss=-3.5113 val_loss=0.0000 scale=1.0000 norm=0.4196\n",
      "[iter 300] loss=-3.6049 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.6135 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.6989 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7288 val_loss=0.0000 scale=0.5000 norm=0.2102\n",
      "[iter 700] loss=-3.7493 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7627 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7884 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 1000] loss=-3.8091 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.8135 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8625 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.8945 val_loss=0.0000 scale=0.5000 norm=0.2055\n",
      "[iter 1400] loss=-3.9227 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-3.9464 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-3.9481 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.9478 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1800] loss=-3.9668 val_loss=0.0000 scale=1.0000 norm=0.4057\n",
      "[iter 1900] loss=-3.9869 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-4.0008 val_loss=0.0000 scale=1.0000 norm=0.4041\n",
      "[iter 2100] loss=-4.0040 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2200] loss=-4.0073 val_loss=0.0000 scale=1.0000 norm=0.4012\n",
      "[iter 2300] loss=-4.0219 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-4.0286 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-4.0428 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-4.0414 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2700] loss=-4.0424 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2800] loss=-4.0576 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2900] loss=-4.0615 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-4.0685 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3100] loss=-4.0727 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3200] loss=-4.0727 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.0773 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:29:05,825] Trial 78 finished with value: 8.02563758864977e-05 and parameters: {'n_estimators': 3381, 'learning_rate': 0.8223851521474387}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1983 val_loss=0.0000 scale=1.0000 norm=0.4365\n",
      "[iter 200] loss=-3.5691 val_loss=0.0000 scale=1.0000 norm=0.4197\n",
      "[iter 300] loss=-3.6934 val_loss=0.0000 scale=1.0000 norm=0.4107\n",
      "[iter 400] loss=-3.7694 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 500] loss=-3.8612 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.8970 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.9193 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.9461 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 900] loss=-3.9670 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.9682 val_loss=0.0000 scale=1.0000 norm=0.4060\n",
      "[iter 1100] loss=-4.0589 val_loss=0.0000 scale=1.0000 norm=0.4022\n",
      "[iter 1200] loss=-4.0726 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0958 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-4.1061 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-4.1069 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1600] loss=-4.1209 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.1255 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1368 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1900] loss=-4.1634 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1760 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.1809 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.1832 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1836 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1902 val_loss=0.0000 scale=0.5000 norm=0.1975\n",
      "[iter 2500] loss=-4.2034 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.2028 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.2081 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.2160 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.2204 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:29:51,932] Trial 79 finished with value: 6.0917083745245615e-05 and parameters: {'n_estimators': 2915, 'learning_rate': 0.7318332299079308}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2501 val_loss=0.0000 scale=0.5000 norm=0.2187\n",
      "[iter 200] loss=-3.5815 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 300] loss=-3.6241 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.6548 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.6531 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 600] loss=-3.7164 val_loss=0.0000 scale=0.5000 norm=0.2085\n",
      "[iter 700] loss=-3.7221 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7504 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7828 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.8483 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.8561 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8771 val_loss=0.0000 scale=1.0000 norm=0.4051\n",
      "[iter 1300] loss=-3.8881 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.9136 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-3.9505 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.9463 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.9468 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.9609 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-3.9695 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.9689 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.9702 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9720 val_loss=0.0000 scale=1.0000 norm=0.4043\n",
      "[iter 2300] loss=-3.9830 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.9862 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.0025 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-4.0086 val_loss=0.0000 scale=0.1250 norm=0.0500\n",
      "[iter 2700] loss=-4.0267 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2800] loss=-4.0390 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:30:42,033] Trial 80 finished with value: 7.446592910336113e-05 and parameters: {'n_estimators': 2868, 'learning_rate': 0.7117063646737659}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1553 val_loss=0.0000 scale=1.0000 norm=0.4374\n",
      "[iter 200] loss=-3.4812 val_loss=0.0000 scale=0.5000 norm=0.2135\n",
      "[iter 300] loss=-3.6867 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 400] loss=-3.7772 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 500] loss=-3.7861 val_loss=0.0000 scale=0.5000 norm=0.2056\n",
      "[iter 600] loss=-3.8549 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 700] loss=-3.8817 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.9338 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 900] loss=-3.9530 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.9694 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.9728 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-3.9780 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-3.9830 val_loss=0.0000 scale=0.0156 norm=0.0064\n",
      "[iter 1400] loss=-4.0011 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-4.0043 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-4.0340 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1700] loss=-4.0476 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-4.0542 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-4.0736 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.0846 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.1031 val_loss=0.0000 scale=1.0000 norm=0.3971\n",
      "[iter 2200] loss=-4.1167 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1221 val_loss=0.0000 scale=0.0156 norm=0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:31:19,592] Trial 81 finished with value: 6.997802458431574e-05 and parameters: {'n_estimators': 2328, 'learning_rate': 0.7427032624370974}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0233 val_loss=0.0000 scale=1.0000 norm=0.4355\n",
      "[iter 200] loss=-3.3902 val_loss=0.0000 scale=1.0000 norm=0.4148\n",
      "[iter 300] loss=-3.5148 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.5387 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.5675 val_loss=0.0000 scale=1.0000 norm=0.4066\n",
      "[iter 600] loss=-3.6544 val_loss=0.0000 scale=0.5000 norm=0.2010\n",
      "[iter 700] loss=-3.6612 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.7133 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 900] loss=-3.7355 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1000] loss=-3.7537 val_loss=0.0000 scale=0.5000 norm=0.1983\n",
      "[iter 1100] loss=-3.7844 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-3.8172 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-3.8275 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1400] loss=-3.8305 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1500] loss=-3.8402 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-3.8533 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-3.8653 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-3.8769 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-3.9011 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-3.9145 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-3.9271 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-3.9273 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-3.9439 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-3.9536 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-3.9612 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2600] loss=-3.9604 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-3.9636 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-3.9690 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2900] loss=-3.9833 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-3.9915 val_loss=0.0000 scale=0.2500 norm=0.0976\n",
      "[iter 3100] loss=-4.0023 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:32:10,089] Trial 82 finished with value: 9.57702453706896e-05 and parameters: {'n_estimators': 3158, 'learning_rate': 0.7825340809793907}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1625 val_loss=0.0000 scale=0.2500 norm=0.1088\n",
      "[iter 200] loss=-3.3624 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.4412 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 400] loss=-3.4939 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.5017 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.5439 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.5641 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.6219 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.6249 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6252 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7115 val_loss=0.0000 scale=0.5000 norm=0.2058\n",
      "[iter 1200] loss=-3.7594 val_loss=0.0000 scale=0.5000 norm=0.2048\n",
      "[iter 1300] loss=-3.7890 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.8015 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8026 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-3.8081 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.8495 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8670 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.8664 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2000] loss=-3.8781 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.8790 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.9102 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.9143 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.9176 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2500] loss=-3.9238 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2600] loss=-3.9249 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2700] loss=-3.9405 val_loss=0.0000 scale=1.0000 norm=0.4073\n",
      "[iter 2800] loss=-3.9412 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2900] loss=-3.9410 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-3.9560 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:33:01,972] Trial 83 finished with value: 8.353896395259542e-05 and parameters: {'n_estimators': 3057, 'learning_rate': 0.7001896198270003}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2216 val_loss=0.0000 scale=1.0000 norm=0.4389\n",
      "[iter 200] loss=-3.5818 val_loss=0.0000 scale=1.0000 norm=0.4209\n",
      "[iter 300] loss=-3.7556 val_loss=0.0000 scale=0.5000 norm=0.2042\n",
      "[iter 400] loss=-3.8286 val_loss=0.0000 scale=0.1250 norm=0.0507\n",
      "[iter 500] loss=-3.8458 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 600] loss=-3.8601 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.9255 val_loss=0.0000 scale=0.2500 norm=0.1017\n",
      "[iter 800] loss=-3.9498 val_loss=0.0000 scale=0.5000 norm=0.2030\n",
      "[iter 900] loss=-3.9978 val_loss=0.0000 scale=0.5000 norm=0.2017\n",
      "[iter 1000] loss=-4.0451 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-4.0677 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-4.0795 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-4.0916 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.1061 val_loss=0.0000 scale=0.0625 norm=0.0250\n",
      "[iter 1500] loss=-4.1114 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.1190 val_loss=0.0000 scale=0.2500 norm=0.0996\n",
      "[iter 1700] loss=-4.1241 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-4.1420 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-4.1560 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2000] loss=-4.1722 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-4.1724 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.1889 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1927 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1974 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-4.2078 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.2172 val_loss=0.0000 scale=1.0000 norm=0.3947\n",
      "[iter 2700] loss=-4.2284 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.2381 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2900] loss=-4.2503 val_loss=0.0000 scale=0.0156 norm=0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:33:48,720] Trial 84 finished with value: 5.9590531792827754e-05 and parameters: {'n_estimators': 2952, 'learning_rate': 0.737031626607514}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1334 val_loss=0.0000 scale=0.5000 norm=0.2095\n",
      "[iter 200] loss=-3.3957 val_loss=0.0000 scale=0.0156 norm=0.0064\n",
      "[iter 300] loss=-3.3953 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 400] loss=-3.4334 val_loss=0.0000 scale=1.0000 norm=0.4141\n",
      "[iter 500] loss=-3.5186 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.5299 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.5682 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.5684 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 900] loss=-3.6145 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6449 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6620 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.6753 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.6713 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7228 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7281 val_loss=0.0000 scale=2.0000 norm=0.7969\n",
      "[iter 1600] loss=-3.7490 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 1700] loss=-3.7731 val_loss=0.0000 scale=0.0625 norm=0.0248\n",
      "[iter 1800] loss=-3.7941 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-3.8188 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2000] loss=-3.8187 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2100] loss=-3.8352 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-3.8378 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2300] loss=-3.8479 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2400] loss=-3.8507 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-3.8493 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2600] loss=-3.8592 val_loss=0.0000 scale=0.2500 norm=0.0982\n",
      "[iter 2700] loss=-3.8734 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-3.8914 val_loss=0.0000 scale=1.0000 norm=0.3868\n",
      "[iter 2900] loss=-3.9092 val_loss=0.0000 scale=0.0039 norm=0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:34:39,373] Trial 85 finished with value: 8.618119997675313e-05 and parameters: {'n_estimators': 2925, 'learning_rate': 0.6598736125313558}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1834 val_loss=0.0000 scale=0.5000 norm=0.2193\n",
      "[iter 200] loss=-3.3746 val_loss=0.0000 scale=0.2500 norm=0.1076\n",
      "[iter 300] loss=-3.4062 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 400] loss=-3.4539 val_loss=0.0000 scale=0.5000 norm=0.2162\n",
      "[iter 500] loss=-3.5473 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 600] loss=-3.5559 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 700] loss=-3.6367 val_loss=0.0000 scale=0.5000 norm=0.2094\n",
      "[iter 800] loss=-3.6523 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.6712 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.6923 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.6974 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7195 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-3.7301 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-3.7336 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7291 val_loss=0.0000 scale=0.1250 norm=0.0521\n",
      "[iter 1600] loss=-3.7336 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.7310 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.7674 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.7679 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-3.7904 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2100] loss=-3.8000 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2200] loss=-3.8163 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2300] loss=-3.8150 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2400] loss=-3.8204 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:35:22,742] Trial 86 finished with value: 0.00011073783572277654 and parameters: {'n_estimators': 2467, 'learning_rate': 0.6113838875675571}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1591 val_loss=0.0000 scale=1.0000 norm=0.4344\n",
      "[iter 200] loss=-3.5637 val_loss=0.0000 scale=1.0000 norm=0.4204\n",
      "[iter 300] loss=-3.6432 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 400] loss=-3.7164 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 500] loss=-3.8134 val_loss=0.0000 scale=1.0000 norm=0.4092\n",
      "[iter 600] loss=-3.8824 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.9267 val_loss=0.0000 scale=0.5000 norm=0.2032\n",
      "[iter 800] loss=-3.9528 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.9942 val_loss=0.0000 scale=0.1250 norm=0.0502\n",
      "[iter 1000] loss=-3.9987 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-4.0360 val_loss=0.0000 scale=0.5000 norm=0.2018\n",
      "[iter 1200] loss=-4.0676 val_loss=0.0000 scale=1.0000 norm=0.4016\n",
      "[iter 1300] loss=-4.0796 val_loss=0.0000 scale=0.5000 norm=0.2004\n",
      "[iter 1400] loss=-4.0944 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.0955 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.1079 val_loss=0.0000 scale=1.0000 norm=0.3990\n",
      "[iter 1700] loss=-4.1189 val_loss=0.0000 scale=0.0156 norm=0.0063\n",
      "[iter 1800] loss=-4.1260 val_loss=0.0000 scale=0.5000 norm=0.2012\n",
      "[iter 1900] loss=-4.1354 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1584 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-4.1655 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.1678 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1782 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1845 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2500] loss=-4.1965 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2600] loss=-4.2029 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.2050 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:36:03,892] Trial 87 finished with value: 7.148985072435329e-05 and parameters: {'n_estimators': 2730, 'learning_rate': 0.7492876759173878}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1361 val_loss=0.0000 scale=1.0000 norm=0.4280\n",
      "[iter 200] loss=-3.4023 val_loss=0.0000 scale=0.0625 norm=0.0266\n",
      "[iter 300] loss=-3.4404 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.5061 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 500] loss=-3.6063 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6600 val_loss=0.0000 scale=0.2500 norm=0.1031\n",
      "[iter 700] loss=-3.7024 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.7051 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7025 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7121 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7090 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7604 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7645 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.7845 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7869 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.7879 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.8134 val_loss=0.0000 scale=0.2500 norm=0.1029\n",
      "[iter 1800] loss=-3.8553 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.8691 val_loss=0.0000 scale=0.0625 norm=0.0254\n",
      "[iter 2000] loss=-3.8773 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.8858 val_loss=0.0000 scale=0.0078 norm=0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:36:39,668] Trial 88 finished with value: 9.824205832323375e-05 and parameters: {'n_estimators': 2103, 'learning_rate': 0.7177778855937231}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1806 val_loss=0.0000 scale=1.0000 norm=0.4261\n",
      "[iter 200] loss=-3.3694 val_loss=0.0000 scale=1.0000 norm=0.4273\n",
      "[iter 300] loss=-3.5493 val_loss=0.0000 scale=0.5000 norm=0.2110\n",
      "[iter 400] loss=-3.5988 val_loss=0.0000 scale=2.0000 norm=0.8331\n",
      "[iter 500] loss=-3.6715 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 600] loss=-3.7261 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7502 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7783 val_loss=0.0000 scale=1.0000 norm=0.4079\n",
      "[iter 900] loss=-3.8140 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1000] loss=-3.8327 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.8415 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8562 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.8564 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.9040 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-3.9285 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.9380 val_loss=0.0000 scale=0.1250 norm=0.0496\n",
      "[iter 1700] loss=-3.9616 val_loss=0.0000 scale=2.0000 norm=0.7877\n",
      "[iter 1800] loss=-3.9854 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1900] loss=-4.0007 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2000] loss=-4.0170 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-4.0230 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.0301 val_loss=0.0000 scale=0.1250 norm=0.0492\n",
      "[iter 2300] loss=-4.0426 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.0522 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.0592 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.0762 val_loss=0.0000 scale=0.2500 norm=0.0984\n",
      "[iter 2700] loss=-4.0838 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-4.0921 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.0993 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.1008 val_loss=0.0000 scale=1.0000 norm=0.3938\n",
      "[iter 3100] loss=-4.1061 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3200] loss=-4.1191 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.1190 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.1240 val_loss=0.0000 scale=0.2500 norm=0.0984\n",
      "[iter 3500] loss=-4.1313 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3600] loss=-4.1452 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3700] loss=-4.1528 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:37:40,023] Trial 89 finished with value: 7.430939391691216e-05 and parameters: {'n_estimators': 3741, 'learning_rate': 0.7688374079565132}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2110 val_loss=0.0000 scale=0.5000 norm=0.2106\n",
      "[iter 200] loss=-3.4334 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.4328 val_loss=0.0000 scale=1.0000 norm=0.4180\n",
      "[iter 400] loss=-3.5133 val_loss=0.0000 scale=1.0000 norm=0.4193\n",
      "[iter 500] loss=-3.5447 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.6510 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.6928 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7007 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.7298 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.7688 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7816 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7963 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-3.8136 val_loss=0.0000 scale=0.0625 norm=0.0249\n",
      "[iter 1400] loss=-3.8354 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.8344 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8630 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.8935 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1800] loss=-3.8992 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9052 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-3.9176 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-3.9234 val_loss=0.0000 scale=0.2500 norm=0.0999\n",
      "[iter 2200] loss=-3.9460 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-3.9472 val_loss=0.0000 scale=1.0000 norm=0.3951\n",
      "[iter 2400] loss=-3.9701 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-3.9892 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-3.9952 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2700] loss=-4.0041 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-4.0212 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.0218 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3000] loss=-4.0277 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3100] loss=-4.0287 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.0280 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3300] loss=-4.0355 val_loss=0.0000 scale=0.5000 norm=0.1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:38:36,113] Trial 90 finished with value: 7.943202008054154e-05 and parameters: {'n_estimators': 3314, 'learning_rate': 0.6444342669554494}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2953 val_loss=0.0000 scale=1.0000 norm=0.4287\n",
      "[iter 200] loss=-3.5971 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 300] loss=-3.6563 val_loss=0.0000 scale=1.0000 norm=0.4178\n",
      "[iter 400] loss=-3.6805 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.7384 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7584 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7809 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.7792 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.8338 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.8305 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-3.8787 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-3.9117 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-3.9273 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-3.9325 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.9357 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.9367 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.9478 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-3.9586 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-3.9798 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2000] loss=-3.9819 val_loss=0.0000 scale=0.2500 norm=0.0993\n",
      "[iter 2100] loss=-3.9893 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.0232 val_loss=0.0000 scale=1.0000 norm=0.3914\n",
      "[iter 2300] loss=-4.0295 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2400] loss=-4.0290 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.0335 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2600] loss=-4.0533 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.0697 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-4.0801 val_loss=0.0000 scale=0.2500 norm=0.0981\n",
      "[iter 2900] loss=-4.0820 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3000] loss=-4.0908 val_loss=0.0000 scale=1.0000 norm=0.3928\n",
      "[iter 3100] loss=-4.0951 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3200] loss=-4.1026 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3300] loss=-4.1065 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.1058 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3500] loss=-4.1051 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3600] loss=-4.1097 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3700] loss=-4.1244 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3800] loss=-4.1233 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3900] loss=-4.1323 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 4000] loss=-4.1396 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4100] loss=-4.1442 val_loss=0.0000 scale=0.5000 norm=0.1947\n",
      "[iter 4200] loss=-4.1587 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4300] loss=-4.1607 val_loss=0.0000 scale=0.0039 norm=0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:39:50,558] Trial 91 finished with value: 6.181864635322796e-05 and parameters: {'n_estimators': 4352, 'learning_rate': 0.6775163425742945}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1970 val_loss=0.0000 scale=1.0000 norm=0.4367\n",
      "[iter 200] loss=-3.4528 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.4520 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 400] loss=-3.4923 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 500] loss=-3.4953 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 600] loss=-3.6462 val_loss=0.0000 scale=1.0000 norm=0.4210\n",
      "[iter 700] loss=-3.7578 val_loss=0.0000 scale=0.0156 norm=0.0066\n",
      "[iter 800] loss=-3.8207 val_loss=0.0000 scale=1.0000 norm=0.4192\n",
      "[iter 900] loss=-3.8471 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 1000] loss=-3.9167 val_loss=0.0000 scale=0.1250 norm=0.0512\n",
      "[iter 1100] loss=-3.9359 val_loss=0.0000 scale=1.0000 norm=0.4067\n",
      "[iter 1200] loss=-3.9946 val_loss=0.0000 scale=0.5000 norm=0.2018\n",
      "[iter 1300] loss=-4.0152 val_loss=0.0000 scale=0.5000 norm=0.2008\n",
      "[iter 1400] loss=-4.0473 val_loss=0.0000 scale=1.0000 norm=0.4001\n",
      "[iter 1500] loss=-4.0660 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.0760 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1700] loss=-4.0806 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.0968 val_loss=0.0000 scale=0.5000 norm=0.1983\n",
      "[iter 1900] loss=-4.1129 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.1136 val_loss=0.0000 scale=1.0000 norm=0.3954\n",
      "[iter 2100] loss=-4.1239 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.1453 val_loss=0.0000 scale=1.0000 norm=0.3928\n",
      "[iter 2300] loss=-4.1553 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1587 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2500] loss=-4.1626 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2600] loss=-4.1721 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.1772 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.1848 val_loss=0.0000 scale=0.0312 norm=0.0122\n",
      "[iter 2900] loss=-4.1926 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.1965 val_loss=0.0000 scale=0.5000 norm=0.1939\n",
      "[iter 3100] loss=-4.2032 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.2090 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3300] loss=-4.2160 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.2315 val_loss=0.0000 scale=0.1250 norm=0.0485\n",
      "[iter 3500] loss=-4.2381 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3600] loss=-4.2507 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3700] loss=-4.2610 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 3800] loss=-4.2641 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3900] loss=-4.2663 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4000] loss=-4.2774 val_loss=0.0000 scale=1.0000 norm=0.3838\n",
      "[iter 4100] loss=-4.2867 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4200] loss=-4.2938 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4300] loss=-4.3074 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4400] loss=-4.3079 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 4500] loss=-4.3099 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:41:01,471] Trial 92 finished with value: 6.280523824940109e-05 and parameters: {'n_estimators': 4540, 'learning_rate': 0.7268490169745527}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0708 val_loss=0.0000 scale=2.0000 norm=0.8575\n",
      "[iter 200] loss=-3.1711 val_loss=0.0000 scale=0.0020 norm=0.0008\n",
      "[iter 300] loss=-3.2862 val_loss=0.0000 scale=1.0000 norm=0.4209\n",
      "[iter 400] loss=-3.4861 val_loss=0.0000 scale=0.5000 norm=0.2101\n",
      "[iter 500] loss=-3.5437 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 600] loss=-3.5448 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 700] loss=-3.5832 val_loss=0.0000 scale=1.0000 norm=0.4193\n",
      "[iter 800] loss=-3.6189 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.6437 val_loss=0.0000 scale=1.0000 norm=0.4178\n",
      "[iter 1000] loss=-3.7458 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.7440 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.7470 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.7700 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 1400] loss=-3.7834 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-3.7822 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.7980 val_loss=0.0000 scale=1.0000 norm=0.4237\n",
      "[iter 1700] loss=-3.8548 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.8526 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.8840 val_loss=0.0000 scale=1.0000 norm=0.4104\n",
      "[iter 2000] loss=-3.9114 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2100] loss=-3.9313 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2200] loss=-3.9505 val_loss=0.0000 scale=0.0312 norm=0.0128\n",
      "[iter 2300] loss=-3.9548 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2400] loss=-3.9769 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2500] loss=-3.9971 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-3.9998 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2700] loss=-4.0002 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2800] loss=-3.9998 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 2900] loss=-4.0082 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3000] loss=-4.0212 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 3100] loss=-4.0286 val_loss=0.0000 scale=0.2500 norm=0.1005\n",
      "[iter 3200] loss=-4.0326 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:41:54,668] Trial 93 finished with value: 8.7675751860974e-05 and parameters: {'n_estimators': 3214, 'learning_rate': 0.8061501990259679}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1393 val_loss=0.0000 scale=1.0000 norm=0.4294\n",
      "[iter 200] loss=-3.5085 val_loss=0.0000 scale=1.0000 norm=0.4207\n",
      "[iter 300] loss=-3.7019 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 400] loss=-3.7460 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.8313 val_loss=0.0000 scale=1.0000 norm=0.4041\n",
      "[iter 600] loss=-3.8727 val_loss=0.0000 scale=0.0625 norm=0.0256\n",
      "[iter 700] loss=-3.9101 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-3.9777 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.9996 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-4.0167 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-4.0308 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-4.0360 val_loss=0.0000 scale=1.0000 norm=0.3974\n",
      "[iter 1300] loss=-4.0470 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.0507 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-4.0658 val_loss=0.0000 scale=0.2500 norm=0.0999\n",
      "[iter 1600] loss=-4.0924 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1700] loss=-4.0950 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 1800] loss=-4.1074 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-4.1133 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2000] loss=-4.1164 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2100] loss=-4.1281 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2200] loss=-4.1328 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1321 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.1365 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.1508 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.1504 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.1576 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.1622 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.1732 val_loss=0.0000 scale=0.5000 norm=0.1972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:42:41,181] Trial 94 finished with value: 7.086334977124509e-05 and parameters: {'n_estimators': 2983, 'learning_rate': 0.7604872446925405}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1907 val_loss=0.0000 scale=1.0000 norm=0.4418\n",
      "[iter 200] loss=-3.4622 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 300] loss=-3.6019 val_loss=0.0000 scale=1.0000 norm=0.4214\n",
      "[iter 400] loss=-3.6741 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.7145 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.7128 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.7215 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 800] loss=-3.8073 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.8173 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.8159 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-3.8183 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-3.8277 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-3.8537 val_loss=0.0000 scale=0.1250 norm=0.0504\n",
      "[iter 1400] loss=-3.8663 val_loss=0.0000 scale=0.2500 norm=0.1017\n",
      "[iter 1500] loss=-3.8754 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-3.8934 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-3.9027 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1800] loss=-3.9389 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1900] loss=-3.9397 val_loss=0.0000 scale=0.2500 norm=0.0999\n",
      "[iter 2000] loss=-3.9443 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-3.9547 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2200] loss=-3.9549 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2300] loss=-3.9556 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-3.9719 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-3.9918 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 2600] loss=-3.9910 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.0164 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2800] loss=-4.0271 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2900] loss=-4.0288 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 3000] loss=-4.0426 val_loss=0.0000 scale=0.5000 norm=0.1987\n",
      "[iter 3100] loss=-4.0653 val_loss=0.0000 scale=1.0000 norm=0.3948\n",
      "[iter 3200] loss=-4.0728 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 3300] loss=-4.0707 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3400] loss=-4.0821 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 3500] loss=-4.0868 val_loss=0.0000 scale=0.0039 norm=0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:43:42,249] Trial 95 finished with value: 7.542236594236965e-05 and parameters: {'n_estimators': 3503, 'learning_rate': 0.7019280902877023}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1798 val_loss=0.0000 scale=1.0000 norm=0.4362\n",
      "[iter 200] loss=-3.5644 val_loss=0.0000 scale=1.0000 norm=0.4322\n",
      "[iter 300] loss=-3.7269 val_loss=0.0000 scale=1.0000 norm=0.4246\n",
      "[iter 400] loss=-3.8399 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 500] loss=-3.8517 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.9084 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 700] loss=-3.9342 val_loss=0.0000 scale=0.0078 norm=0.0033\n",
      "[iter 800] loss=-3.9681 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.9886 val_loss=0.0000 scale=1.0000 norm=0.4100\n",
      "[iter 1000] loss=-4.0406 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1100] loss=-4.0491 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1200] loss=-4.0572 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1300] loss=-4.0767 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1400] loss=-4.0876 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1500] loss=-4.0990 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.1233 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1700] loss=-4.1257 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1343 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1900] loss=-4.1411 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2000] loss=-4.1495 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.1566 val_loss=0.0000 scale=0.1250 norm=0.0497\n",
      "[iter 2200] loss=-4.1708 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2300] loss=-4.1707 val_loss=0.0000 scale=0.0156 norm=0.0062\n",
      "[iter 2400] loss=-4.1789 val_loss=0.0000 scale=0.2500 norm=0.0999\n",
      "[iter 2500] loss=-4.1945 val_loss=0.0000 scale=0.0156 norm=0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:44:23,675] Trial 96 finished with value: 6.817186313431997e-05 and parameters: {'n_estimators': 2565, 'learning_rate': 0.7289391104879649}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.0731 val_loss=0.0000 scale=1.0000 norm=0.4319\n",
      "[iter 200] loss=-3.4177 val_loss=0.0000 scale=1.0000 norm=0.4255\n",
      "[iter 300] loss=-3.6801 val_loss=0.0000 scale=0.5000 norm=0.2106\n",
      "[iter 400] loss=-3.7818 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 500] loss=-3.8871 val_loss=0.0000 scale=1.0000 norm=0.4094\n",
      "[iter 600] loss=-3.8958 val_loss=0.0000 scale=0.5000 norm=0.2050\n",
      "[iter 700] loss=-3.9593 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 800] loss=-4.0234 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-4.0608 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-4.0916 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1100] loss=-4.1086 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-4.1165 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.1353 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-4.1549 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.1566 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.1781 val_loss=0.0000 scale=0.5000 norm=0.1980\n",
      "[iter 1700] loss=-4.1846 val_loss=0.0000 scale=0.5000 norm=0.1975\n",
      "[iter 1800] loss=-4.1881 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.1990 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2000] loss=-4.2030 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2100] loss=-4.2248 val_loss=0.0000 scale=0.5000 norm=0.1949\n",
      "[iter 2200] loss=-4.2290 val_loss=0.0000 scale=0.0156 norm=0.0061\n",
      "[iter 2300] loss=-4.2283 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.2447 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-4.2521 val_loss=0.0000 scale=0.5000 norm=0.1942\n",
      "[iter 2600] loss=-4.2534 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.2561 val_loss=0.0000 scale=1.0000 norm=0.3882\n",
      "[iter 2800] loss=-4.2566 val_loss=0.0000 scale=0.5000 norm=0.1936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:45:08,743] Trial 97 finished with value: 6.012265665817567e-05 and parameters: {'n_estimators': 2812, 'learning_rate': 0.8605223250694259}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2679 val_loss=0.0000 scale=1.0000 norm=0.4454\n",
      "[iter 200] loss=-3.6281 val_loss=0.0000 scale=0.5000 norm=0.2113\n",
      "[iter 300] loss=-3.6943 val_loss=0.0000 scale=0.5000 norm=0.2081\n",
      "[iter 400] loss=-3.8444 val_loss=0.0000 scale=0.2500 norm=0.1031\n",
      "[iter 500] loss=-3.9581 val_loss=0.0000 scale=1.0000 norm=0.4054\n",
      "[iter 600] loss=-3.9646 val_loss=0.0000 scale=0.0312 norm=0.0127\n",
      "[iter 700] loss=-4.0118 val_loss=0.0000 scale=2.0000 norm=0.8240\n",
      "[iter 800] loss=-4.0302 val_loss=0.0000 scale=0.5000 norm=0.2061\n",
      "[iter 900] loss=-4.0753 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-4.1177 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1100] loss=-4.1449 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-4.1657 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-4.1822 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1400] loss=-4.2038 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.2051 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1600] loss=-4.2106 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.2367 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.2404 val_loss=0.0000 scale=0.5000 norm=0.1975\n",
      "[iter 1900] loss=-4.2467 val_loss=0.0000 scale=0.5000 norm=0.1985\n",
      "[iter 2000] loss=-4.2539 val_loss=0.0000 scale=1.0000 norm=0.3946\n",
      "[iter 2100] loss=-4.2747 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.2896 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2300] loss=-4.2956 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2400] loss=-4.3021 val_loss=0.0000 scale=0.5000 norm=0.1979\n",
      "[iter 2500] loss=-4.3133 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.3230 val_loss=0.0000 scale=0.2500 norm=0.0984\n",
      "[iter 2700] loss=-4.3270 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2800] loss=-4.3344 val_loss=0.0000 scale=0.0078 norm=0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:45:53,392] Trial 98 finished with value: 5.873299663777293e-05 and parameters: {'n_estimators': 2818, 'learning_rate': 0.8890177329061306}. Best is trial 24 with value: 5.592775553556088e-05.\n",
      "C:\\Users\\zhaokaiyang\\AppData\\Local\\Temp\\ipykernel_17896\\2903388670.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.1034 val_loss=0.0000 scale=1.0000 norm=0.4445\n",
      "[iter 200] loss=-3.4664 val_loss=0.0000 scale=1.0000 norm=0.4265\n",
      "[iter 300] loss=-3.6779 val_loss=0.0000 scale=1.0000 norm=0.4176\n",
      "[iter 400] loss=-3.8116 val_loss=0.0000 scale=0.2500 norm=0.1019\n",
      "[iter 500] loss=-3.8833 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 600] loss=-3.9263 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 700] loss=-3.9394 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 800] loss=-3.9692 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.9689 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-3.9851 val_loss=0.0000 scale=0.5000 norm=0.1980\n",
      "[iter 1100] loss=-3.9930 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1200] loss=-4.0091 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1300] loss=-4.0327 val_loss=0.0000 scale=0.2500 norm=0.0988\n",
      "[iter 1400] loss=-4.0706 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1500] loss=-4.0880 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1600] loss=-4.1068 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.1237 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1800] loss=-4.1441 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.1518 val_loss=0.0000 scale=0.0039 norm=0.0015\n",
      "[iter 2000] loss=-4.1561 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.1793 val_loss=0.0000 scale=0.1250 norm=0.0492\n",
      "[iter 2200] loss=-4.1997 val_loss=0.0000 scale=0.2500 norm=0.0980\n",
      "[iter 2300] loss=-4.2159 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2400] loss=-4.2379 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2500] loss=-4.2439 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2600] loss=-4.2476 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2700] loss=-4.2504 val_loss=0.0000 scale=0.0078 norm=0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-01 12:46:39,681] Trial 99 finished with value: 6.35135758007011e-05 and parameters: {'n_estimators': 2755, 'learning_rate': 0.862617381363128}. Best is trial 24 with value: 5.592775553556088e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken:  5557.467022895813\n",
      "Best Parameters:  {'n_estimators': 4140, 'learning_rate': 0.8854198851197161}\n",
      "Best MSE:  5.592775553556088e-05\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    n_estimators = trial.suggest_int('n_estimators', 1000, 5000)\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.001, 0.999)\n",
    "    nbr_reg = NGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "    nbr_reg.fit(X_train, y_train)\n",
    "    y_pred = nbr_reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return mse\n",
    "\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "# Calculate total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time taken: \", total_time)\n",
    "best_params = study.best_params\n",
    "best_mse = study.best_value\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best MSE: \", best_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "373199be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=-0.0899 val_loss=0.0000 scale=1.0000 norm=0.4985\n",
      "[iter 100] loss=-3.2056 val_loss=0.0000 scale=2.0000 norm=0.9080\n",
      "[iter 200] loss=-3.6047 val_loss=0.0000 scale=1.0000 norm=0.4317\n",
      "[iter 300] loss=-3.7028 val_loss=0.0000 scale=0.0312 norm=0.0136\n",
      "[iter 400] loss=-3.7039 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 500] loss=-3.7836 val_loss=0.0000 scale=1.0000 norm=0.4307\n",
      "[iter 600] loss=-3.8493 val_loss=0.0000 scale=0.0039 norm=0.0017\n",
      "[iter 700] loss=-3.8698 val_loss=0.0000 scale=1.0000 norm=0.4319\n",
      "[iter 800] loss=-3.9332 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 900] loss=-3.9690 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1000] loss=-4.0103 val_loss=0.0000 scale=0.0312 norm=0.0130\n",
      "[iter 1100] loss=-4.0276 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1200] loss=-4.0430 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1300] loss=-4.0643 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1400] loss=-4.0989 val_loss=0.0000 scale=0.0039 norm=0.0016\n",
      "[iter 1500] loss=-4.1315 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1600] loss=-4.1611 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1700] loss=-4.1850 val_loss=0.0000 scale=0.0078 norm=0.0032\n",
      "[iter 1800] loss=-4.1939 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 1900] loss=-4.2032 val_loss=0.0000 scale=0.2500 norm=0.0992\n",
      "[iter 2000] loss=-4.2239 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2100] loss=-4.2396 val_loss=0.0000 scale=0.0078 norm=0.0031\n",
      "[iter 2200] loss=-4.2642 val_loss=0.0000 scale=2.0000 norm=0.7930\n",
      "[iter 2300] loss=-4.2894 val_loss=0.0000 scale=0.2500 norm=0.0980\n",
      "[iter 2400] loss=-4.3037 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2500] loss=-4.3260 val_loss=0.0000 scale=1.0000 norm=0.3876\n",
      "[iter 2600] loss=-4.3373 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2700] loss=-4.3386 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2800] loss=-4.3393 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 2900] loss=-4.3477 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3000] loss=-4.3517 val_loss=0.0000 scale=0.0156 norm=0.0060\n",
      "[iter 3100] loss=-4.3569 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3200] loss=-4.3611 val_loss=0.0000 scale=0.5000 norm=0.1909\n",
      "[iter 3300] loss=-4.3657 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3400] loss=-4.3834 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3500] loss=-4.3867 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3600] loss=-4.3925 val_loss=0.0000 scale=0.5000 norm=0.1900\n",
      "[iter 3700] loss=-4.3993 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 3800] loss=-4.4038 val_loss=0.0000 scale=0.2500 norm=0.0948\n",
      "[iter 3900] loss=-4.4070 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4000] loss=-4.4147 val_loss=0.0000 scale=0.0078 norm=0.0030\n",
      "[iter 4100] loss=-4.4209 val_loss=0.0000 scale=0.0078 norm=0.0029\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NGBRegressor(learning_rate=0.885, n_estimators=4140,\n",
       "             random_state=RandomState(MT19937) at 0x1AF05160840)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NGBRegressor</label><div class=\"sk-toggleable__content\"><pre>NGBRegressor(learning_rate=0.885, n_estimators=4140,\n",
       "             random_state=RandomState(MT19937) at 0x1AF05160840)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Base: DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(criterion=&#x27;friedman_mse&#x27;, max_depth=3)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(criterion=&#x27;friedman_mse&#x27;, max_depth=3)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "NGBRegressor(learning_rate=0.885, n_estimators=4140,\n",
       "             random_state=RandomState(MT19937) at 0x1AF05160840)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ngboost import NGBRegressor\n",
    "params_best['n_estimators'] = int(params_best['n_estimators'])\n",
    "nbr_reg= NGBRegressor(**best_params)\n",
    "nbr_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a8bfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= nbr_reg.predict(X_test)\n",
    "y_pred1=nbr_reg.predict(X_val)\n",
    "y_pred2=nbr_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "285f5a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error: 0.004718427788562277\n",
      "mean_squared_error: 4.590750908259144e-05\n",
      "rmse: 0.006775508031328089\n",
      "r2 score: 0.9990792498029739\n"
     ]
    }
   ],
   "source": [
    "print(\"mean_absolute_error:\", mean_absolute_error(y_val, y_pred1))\n",
    "print(\"mean_squared_error:\", mean_squared_error(y_val, y_pred1))\n",
    "print(\"rmse:\", sqrt(mean_squared_error(y_val, y_pred1)))\n",
    "print(\"r2 score:\", r2_score(y_val, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a3f3505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error: 0.003214011654379734\n",
      "mean_squared_error: 2.124320623469828e-05\n",
      "rmse: 0.004609035282431485\n",
      "r2 score: 0.9995657144434325\n"
     ]
    }
   ],
   "source": [
    "print(\"mean_absolute_error:\", mean_absolute_error(y_train, y_pred2))\n",
    "print(\"mean_squared_error:\", mean_squared_error(y_train, y_pred2))\n",
    "print(\"rmse:\", sqrt(mean_squared_error(y_train, y_pred2)))\n",
    "print(\"r2 score:\", r2_score(y_train, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0c9b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error: 0.005018973375091401\n",
      "mean_squared_error: 5.6036682505582246e-05\n",
      "rmse: 0.00748576532530791\n",
      "r2 score: 0.9989061886269456\n"
     ]
    }
   ],
   "source": [
    "print(\"mean_absolute_error:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"mean_squared_error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"rmse:\", sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"r2 score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52059387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "slope, intercept, _, _, _ = stats.linregress(y_test, y_pred)\n",
    "fit_line = intercept + slope * y_test\n",
    "sns.set_style('darkgrid')\n",
    "sns.regplot(x=y_pred, y=y_test, label='True', scatter_kws={'color': 'blue'})\n",
    "sns.regplot(x=y_test, y=y_pred, label='Predicted', scatter_kws={'color': 'orange'})\n",
    "plt.plot(y_test, fit_line, label='Fit', color='green')\n",
    "plt.fill_between(y_test, y_pred - (y_pred - y_test), y_pred + (y_pred - y_test), alpha=0.2, color='red')\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('Y', fontsize=12)\n",
    "plt.legend()\n",
    "plt.text(0.95, 0.05, 'Ngboostâ€”Optuna', transform=plt.gca().transAxes, ha='right', color='black')\n",
    "plt.text(0.4, 0.9, f'RÂ² Score: {r2:.4f}', transform=plt.gca().transAxes, ha='left', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b455e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
